{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Private AI Workflows with LangChain and Ollama\n",
    "\n",
    "This notebook demonstrates how to integrate LangChain with Ollama to run language models locally for privacy-preserving AI workflows.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install required packages:\n",
    "```bash\n",
    "pip install langchain langchain-community langchain-ollama\n",
    "```\n",
    "\n",
    "Install Ollama:\n",
    "- For macOS: Download from [ollama.com](https://ollama.com)\n",
    "- For Linux: `curl -fsSL https://ollama.com/install.sh | sh`\n",
    "- For Windows: Download Windows (Preview) from [ollama.com](https://ollama.com)\n",
    "\n",
    "Start Ollama server:\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "Pull a model (example):\n",
    "```bash\n",
    "ollama pull qwen3:0.6b\n",
    "```\n",
    "\n",
    "## What You'll Learn\n",
    "- Set up LangChain with Ollama for local AI\n",
    "- Use chat and completion models  \n",
    "- Build LangChain chains for complex workflows\n",
    "- Work with embeddings for semantic search\n",
    "- Build a question-answering system with RAG\n",
    "\n",
    "## Why Local AI Matters\n",
    "\n",
    "AI models are changing data science projects by automating feature engineering, summarizing datasets, generating reports, and even writing code to examine or clean data.\n",
    "\n",
    "However, using popular APIs like OpenAI or Anthropic can introduce serious privacy risks, especially when handling regulated data such as medical records, legal documents, or internal company knowledge.\n",
    "\n",
    "When data privacy is important, running models locally ensures full control. Nothing leaves your machine, so you manage all inputs, outputs, and processing securely.\n",
    "\n",
    "That's where [LangChain](https://www.langchain.com/) and [Ollama](https://ollama.com/) come in. LangChain provides the framework to build AI applications. Ollama lets you run open-source models locally.\n",
    "\n",
    "## Introduction to Ollama and LangChain\n",
    "\n",
    "### What is Ollama?\n",
    "\n",
    "Ollama is an open-source tool that makes it easy to run large language models locally. It offers a simple CLI and REST API for downloading and interacting with popular models like Llama, Mistral, DeepSeek, and Gemmaâ€”no complex setup required.\n",
    "\n",
    "Since Ollama doesn't depend on external APIs, it is ideal for sensitive data or limited-connectivity environments.\n",
    "\n",
    "### What is LangChain?\n",
    "\n",
    "LangChain is a framework for creating AI applications using language models. Rather than writing custom code for model interactions, response handling, and error management, you can use LangChain's ready-made components to build applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-community langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama needs to be installed separately since it's a standalone service that runs locally:\n",
    "\n",
    "- For macOS: Download from [ollama.com](https://ollama.com) - this installs both the CLI tool and service\n",
    "- For Linux: `curl -fsSL https://ollama.com/install.sh | sh` - this script sets up both the binary and system service\n",
    "- For Windows: Download Windows (Preview) from [ollama.com](https://ollama.com) - still in preview mode with some limitations\n",
    "\n",
    "Start the Ollama server:\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "The server will run in the background, handling model loading and inference requests.\n",
    "\n",
    "### Pulling Models with Ollama\n",
    "\n",
    "Before using any model with LangChain, you need to pull it to your local machine with Ollama:\n",
    "\n",
    "```bash\n",
    "ollama pull qwen3:0.6b\n",
    "```\n",
    "\n",
    "Once it is downloaded, you can serve the model with the following command:\n",
    "\n",
    "```bash\n",
    "ollama run qwen3:0.6b\n",
    "```\n",
    "\n",
    "The model size has a large impact on performance and resource requirements:\n",
    "\n",
    "- Smaller models (7B-8B) run well on most modern computers with 16GB+ RAM\n",
    "- Medium models (13B-34B) need more RAM or GPU acceleration\n",
    "- Large models (70B+) typically require a dedicated GPU with 24GB+ VRAM\n",
    "\n",
    "For a full list of models you can serve locally, check out [the Ollama model library](https://ollama.com/search). Before pulling a model and potentially waste your hardware resources, check out [the VRAM calculator](https://apxml.com/tools/vram-calculator) that tells you if you can run a specific model on your machine:\n",
    "\n",
    "\n",
    "### Basic Chat Integration\n",
    "\n",
    "Once you have a model downloaded, you need to connect LangChain to Ollama for actual AI interactions. LangChain uses dedicated classes that handle the communication between your Python code and the Ollama service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the chat model with specific configurations\n",
    "chat_model = ChatOllama(\n",
    "    model=\"qwen3:0.6b\",\n",
    "    temperature=0.5,\n",
    "    base_url=\"http://localhost:11434\",  # Can be changed for remote Ollama instances\n",
    ")\n",
    "\n",
    "# Create a conversation with system and user messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful coding assistant specialized in Python.\"),\n",
    "    HumanMessage(content=\"Write a recursive Fibonacci function with memoization.\")\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_async():\n",
    "    response = await chat_model.ainvoke(messages)\n",
    "    return response.content\n",
    "\n",
    "# In async context\n",
    "result = await generate_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( result[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize the LLM with specific options\n",
    "llm = OllamaLLM(\n",
    "    model=\"qwen3:0.6b\",\n",
    ")\n",
    "\n",
    "# Generate text from a prompt\n",
    "text = \"\"\"\n",
    "Write a quick sort algorithm in Python with detailed comments:\n",
    "```{python}\n",
    "def quicksort(\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(text)\n",
    "print(response[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(\"Explain quantum computing in three sentences:\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(\n",
    "    model=\"qwen3:0.6b\", # Example model, can be any model supported by Ollama\n",
    "    temperature=0.7,      # Controls randomness (0.0 = deterministic, 1.0 = creative)\n",
    "    stop=[\"```\", \"###\"],  # Stop sequences to end generation\n",
    "    repeat_penalty=1.1,   # Penalizes repetition (>1.0 reduces repetition)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details about these parameters:\n",
    "\n",
    "- `model`: Specifies the language model to use.\n",
    "- `temperature`: Controls randomness; lower = more focused, higher = more creative.\n",
    "- `stop`: Defines stop sequences that terminate generation early. Once one of these sequences is produced, the model stops generating further tokens.\n",
    "- `repeat_penalty`: Penalizes repeated tokens to reduce redundancy. Values greater than 1.0 discourage the model from repeating itself.\n",
    "\n",
    "Parameter recommendations:\n",
    "\n",
    "- For factual or technical responses: Lower `temperature` (0.1-0.3) and higher `repeat_penalty` (1.1-1.2)\n",
    "- For creative writing: Higher `temperature` (0.7-0.9)\n",
    "- For code generation: Medium `temperature` (0.3-0.6) with specific `stop` like \"\\`\\`\\`\"\n",
    "\n",
    "The model behavior changes dramatically with these settings. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific writing with precise output\n",
    "scientific_llm = OllamaLLM(model=\"qwen3:0.6b\", temperature=0.1, repeat_penalty=1.2)\n",
    "\n",
    "# Creative storytelling\n",
    "creative_llm = OllamaLLM(model=\"qwen3:0.6b\", temperature=0.9, repeat_penalty=1.0)\n",
    "\n",
    "# Code generation\n",
    "code_llm = OllamaLLM(model=\"codellama\", temperature=0.3, stop=[\"```\", \"def \"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LangChain Chains {#creating-langchain-chains}\n",
    "\n",
    " AI  workflows often involve multiple steps: data validation, prompt formatting, model inference, and output processing. Running these steps manually for each request becomes repetitive and error-prone. \n",
    "\n",
    "LangChain addresses this by chaining steps into sequences to create end-to-end applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create a structured prompt template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert educator.\n",
    "Explain the following concept in simple terms that a beginner would understand.\n",
    "Make sure to provide:\n",
    "1. A clear definition\n",
    "2. A real-world analogy\n",
    "3. A practical example\n",
    "\n",
    "Concept: {concept}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the required LangChain components and create a prompt template. The `PromptTemplate.from_template()` method creates a reusable template with placeholder variables (like `{concept}`) that get filled in at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parser that extracts structured data\n",
    "class JsonOutputParser:\n",
    "    def parse(self, text):\n",
    "        try:\n",
    "            # Find JSON blocks in the text\n",
    "            if \"```json\" in text and \"```\" in text.split(\"```json\")[1]:\n",
    "                json_str = text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "                return json.loads(json_str)\n",
    "            # Try to parse the whole text as JSON\n",
    "            return json.loads(text)\n",
    "        except (json.JSONDecodeError, ValueError, IndexError):\n",
    "            # Fall back to returning the raw text\n",
    "            return {\"raw_output\": text}\n",
    "\n",
    "# Initialize a model instance to be used in the chain\n",
    "llm = OllamaLLM(model=\"qwen3:0.6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a custom output parser. This class attempts to extract JSON from the model's response, handling both code-block format and raw JSON. If parsing fails, it returns the original text wrapped in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a more complex chain\n",
    "chain = (\n",
    "    {\"concept\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Execute the chain with detailed tracking\n",
    "result = chain.invoke(\"Recursive neural networks\")\n",
    "print(result[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Initialize embeddings model with specific parameters\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",  # Specialized embedding model that is also supported by Ollama\n",
    "    base_url=\"http://localhost:11434\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nomic-embed-text` model is designed specifically for creating high-quality text embeddings. Unlike general language models that generate text, embedding models focus solely on converting text into meaningful vector representations.\n",
    "\n",
    "Now let's create an embedding for a sample query and examine its properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for a query\n",
    "query = \"How do neural networks learn?\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "print(f\"Embedding dimension: {len(query_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 768-dimensional vector represents our query in mathematical space. Each dimension captures different semantic features - some might relate to technical concepts, others to question patterns, and so on. Words with similar meanings will have vectors that point in similar directions.\n",
    "\n",
    "\n",
    "Next, we'll create embeddings for multiple documents to demonstrate similarity matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for multiple documents\n",
    "documents = [\n",
    "    \"Neural networks learn through backpropagation\",\n",
    "    \"Transformers use attention mechanisms\",\n",
    "    \"LLMs are trained on text data\"\n",
    "]\n",
    "\n",
    "doc_embeddings = embeddings.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `embed_documents()` method processes multiple texts at once, which is more efficient than calling `embed_query()` repeatedly. This batch processing saves time when working with large document collections.\n",
    "\n",
    "To find which document best matches our query, we need to measure similarity between vectors. Cosine similarity is the standard approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity between vectors\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Find most similar document to query\n",
    "similarities = [cosine_similarity(query_embedding, doc_emb) for doc_emb in doc_embeddings]\n",
    "most_similar_idx = np.argmax(similarities)\n",
    "print(f\"Most similar document: {documents[most_similar_idx]}\")\n",
    "print(f\"Similarity score: {similarities[most_similar_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# Initialize components\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "chat_model = ChatOllama(model=\"qwen3:0.6b\", temperature=0.3)\n",
    "\n",
    "# Sample knowledge base representing project documentation\n",
    "documents = [\n",
    "    Document(page_content=\"Python is a high-level programming language known for its simplicity and readability.\"),\n",
    "    Document(page_content=\"Machine learning algorithms can automatically learn patterns from data without explicit programming.\"),\n",
    "    Document(page_content=\"Data preprocessing involves cleaning, changing, and organizing raw data for analysis.\"),\n",
    "    Document(page_content=\"Neural networks are computational models inspired by biological brain networks.\"),\n",
    "]\n",
    "\n",
    "# Create embeddings for all documents\n",
    "doc_embeddings = embeddings.embed_documents([doc.page_content for doc in documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setup creates a searchable knowledge base from your documents. In production systems, these documents would contain sections from research papers, methodology descriptions, data analysis reports, or code documentation. The embeddings convert each document into vectors that support semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query, top_k=2):\n",
    "    \"\"\"Find the most relevant documents for a query\"\"\"\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = []\n",
    "    for doc_emb in doc_embeddings:\n",
    "        similarity = np.dot(query_embedding, doc_emb) / (\n",
    "            np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)\n",
    "        )\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # Get top-k most similar documents\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return [documents[i] for i in top_indices]\n",
    "\n",
    "# Create RAG prompt template\n",
    "rag_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the question. If the answer isn't in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `similarity_search` function finds documents most relevant to a question using the embeddings we created earlier. The prompt template structures how we present retrieved context to the language model, instructing it to base answers on the provided documents rather than general knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    \"\"\"Generate an answer using retrieved context\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = similarity_search(question, top_k=2)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # Generate answer using context\n",
    "    prompt_text = rag_prompt.format(context=context, question=question)\n",
    "    response = chat_model.invoke([{\"role\": \"user\", \"content\": prompt_text}])\n",
    "\n",
    "    return response.content, relevant_docs\n",
    "\n",
    "# Test the RAG system\n",
    "question = \"What makes Python popular for data science?\"\n",
    "answer, sources = answer_question(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Sources: {[doc.page_content[:50] + '...' for doc in sources]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/khuyentran/codecut_content/codecut_articles/.venv/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
