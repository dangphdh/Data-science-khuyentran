{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction {#introduction}\n",
    "Have you ever searched for something online and been frustrated when the search engine couldn't find what you were looking for, even though you knew it existed? Traditional search relies on exact word matching, so searching for \"Denim for men\" won't return products labeled \"Men's Jeans 32W\" or \"Men's Trendy Jeans.\"\n",
    "Semantic search solves this by understanding meaning and context. It recognizes that \"denim\" relates to \"jeans\" and \"men\" connects to \"32W\" in clothing contexts, returning relevant results despite different wording.\n",
    "This comprehensive Pinecone vector database tutorial will teach you to build a complete semantic search pipeline using Ollama embeddings for local processing. You'll learn to implement a Python vector database solution that combines the power of Pinecone vector database with Ollama embeddings and LangChain orchestration.\n",
    "By the end of this semantic search tutorial, you'll have a fully functional system that processes documents locally and stores them in a scalable vector database for lightning-fast similarity searches.\n",
    "<div class=\"klaviyo-form-VWXSdu\" style=\"margin: 20px;\"></div>\n",
    "## What Is Pinecone? {#what-is-pinecone}\n",
    "[**Pinecone**](https://github.com/pinecone-io) is a vector database designed specifically for storing and searching high-dimensional data. But what does that mean for data scientists?\n",
    "Think of traditional databases like spreadsheets: they store text, numbers, and dates in rows and columns. Vector databases like Pinecone store mathematical representations of data called vectors (arrays of numbers that capture meaning).\n",
    "Here's a simple comparison\n",
    "**Traditional Database:**\n",
    "| ID | Product Name    | Category |\n",
    "|----|-----------------|----------|\n",
    "| 1  | Men's Jeans 32W | Clothing |\n",
    "| 2  | Denim Pants     | Apparel  |\n",
    "**Vector Database:**\n",
    "| ID | Product Vector                    | \n",
    "|----|-----------------------------------|\n",
    "| 1  | [0.2, 0.8, 0.1, 0.9, ...]       |\n",
    "| 2  | [0.3, 0.7, 0.2, 0.8, ...]       |\n",
    "By storing data as vectors, Pinecone enables semantic understanding beyond what traditional databases can achieve. Similar products have similar vector representations, allowing Pinecone to find relevant results based on meaning rather than exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import apply_codecut_style\n",
    "\n",
    "# Create side-by-side comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left plot: Similarity scores (easier to understand)\n",
    "products = [\"Men's Jeans\", \"Denim Pants\", \"Leather Jacket\"]\n",
    "query = \"denim for men\"\n",
    "similarity_scores = [0.92, 0.89, 0.45]\n",
    "colors = [\"#E583B6\", \"#72BEFA\", \"#72FCDB\"]\n",
    "\n",
    "bars = ax1.barh(products, similarity_scores, color=colors, alpha=0.8)\n",
    "ax1.set_xlabel(\"Similarity Score\", fontsize=12, color=\"white\")\n",
    "ax1.set_title(f'Query: \"{query}\"', fontsize=13, color=\"white\")\n",
    "ax1.set_xlim(0, 1.0)\n",
    "\n",
    "# Add score labels on bars\n",
    "for bar, score in zip(bars, similarity_scores, strict=False):\n",
    "    ax1.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "             f\"{score:.2f}\", va=\"center\", fontsize=11, color=\"white\")\n",
    "\n",
    "# Right plot: Simple 2D vector space (same products as left plot)\n",
    "vectors = {\n",
    "    \"Men's Jeans\": [0.8, 0.7],\n",
    "    \"Denim Pants\": [0.75, 0.72], \n",
    "    \"Leather Jacket\": [0.6, 0.4]\n",
    "}\n",
    "\n",
    "for i, (product, pos) in enumerate(vectors.items()):\n",
    "    ax2.scatter(pos[0], pos[1], c=colors[i], s=200, \n",
    "               edgecolors=\"white\", linewidth=2, alpha=0.8)\n",
    "\n",
    "    # Smart label positioning to avoid overlaps\n",
    "    if product == \"Denim Pants\":\n",
    "        offset_x, offset_y, ha = -0.08, -0.03, \"right\"  # Closer to blue dot\n",
    "    elif product == \"Leather Jacket\":\n",
    "        offset_x, offset_y, ha = 0.05, -0.05, \"left\"\n",
    "    else:\n",
    "        offset_x, offset_y, ha = 0.05, 0.05, \"left\"\n",
    "\n",
    "    ax2.text(pos[0] + offset_x, pos[1] + offset_y, product, \n",
    "             fontsize=11, color=\"white\", ha=ha)\n",
    "\n",
    "# Draw similarity grouping for denim products\n",
    "circle = plt.Circle((0.775, 0.71), 0.08, fill=False, \n",
    "                   linestyle=\"--\", color=\"white\", alpha=0.7, linewidth=2)\n",
    "ax2.add_patch(circle)\n",
    "ax2.text(0.85, 0.78, \"Similar\\nProducts\", fontsize=10, color=\"white\", \n",
    "         ha=\"center\", style=\"italic\")\n",
    "\n",
    "ax2.set_xlim(0.2, 0.95)\n",
    "ax2.set_ylim(0.15, 0.85)\n",
    "ax2.set_xlabel(\"Semantic Dimension 1\", fontsize=12, color=\"white\")\n",
    "ax2.set_ylabel(\"Semantic Dimension 2\", fontsize=12, color=\"white\")\n",
    "ax2.set_title(\"Vector Space: Similar Items Cluster Together\", fontsize=13, color=\"white\")\n",
    "\n",
    "# Apply styling to both plots\n",
    "for ax in [ax1, ax2]:\n",
    "    apply_codecut_style(ax)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://codecut.ai/wp-content/uploads/2025/06/output-1.png)\n",
    "\n",
    "The visualization demonstrates semantic search in action where distance between dots represents similarity. The dashed circle shows how \"denim\" products (pink and blue dots) cluster close together, while the leather jacket (green dot) sits farther away, indicating lower semantic similarity.\n",
    "\n",
    "Pinecone stands out among vector databases with its cloud-native design that eliminates infrastructure management:\n",
    "\n",
    "- **Serverless architecture**: No need to manage clusters or configure hardware unlike self-hosted options like Weaviate or Qdrant\n",
    "- **Automatic scaling**: Handles millions of vectors without manual intervention, unlike pgvector which requires PostgreSQL tuning\n",
    "- **Built-in monitoring**: Provides performance metrics and alerts out-of-the-box, reducing operational overhead\n",
    "- **Optimized indexing**: Uses proprietary algorithms for sub-millisecond search across billion-scale datasets\n",
    "\n",
    "## What Are Ollama and LangChain? {#what-are-ollama-and-langchain}\n",
    "\n",
    "[**Ollama**](https://github.com/ollama/ollama) provides free, local LLM hosting with complete data privacy, unlike paid cloud APIs like OpenAI or Claude.\n",
    "\n",
    "[**LangChain**](https://github.com/langchain-ai/langchain) serves as the orchestration framework for building LLM applications efficiently. For comprehensive LangChain fundamentals, see our [LangChain and Ollama guide](https://codecut.ai/private-ai-workflows-langchain-ollama/).\n",
    "\n",
    "## Overview of the Architecture {#overview-of-the-architecture}\n",
    "\n",
    "The architecture follows a straightforward design:\n",
    "\n",
    "- **Ollama**: Generates embeddings locally\n",
    "- **Pinecone**: Stores vectors and performs similarity search\n",
    "- **LangChain**: Orchestrates the entire pipeline\n",
    "\n",
    "![](https://codecut.ai/wp-content/uploads/2025/06/diagram-export-6-22-2025-12_38_51-PM-1.png)\n",
    "\n",
    "## Step-by-Step Implementation {#step-by-step-implementation}\n",
    "\n",
    "### Ollama Setup {#ollama-setup}\n",
    "\n",
    "We can setup Ollama locally by downloading and installing it first. For Linux users, run the installation script:\n",
    "\n",
    "```bash\n",
    "# For Linux users\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "Once installation completes, start the Ollama server locally.\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "Next, download a model specifically designed for generating embeddings. We'll use `mxbai-embed-large`, which is optimized for semantic search tasks.\n",
    "\n",
    "```bash\n",
    "ollama pull mxbai-embed-large\n",
    "```\n",
    "\n",
    "With Ollama installed and the embedding model downloaded, we're ready to start coding.\n",
    "\n",
    "### Loading Text Data {#loading-text-data}\n",
    "\n",
    "Next, we'll prepare our text data for embedding generation. For this tutorial, we'll work with PDF documents stored in a `data` folder using LangChain's document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "\n",
    "# read/load the pdf document from the directory\n",
    "def read_doc(directory):\n",
    "    file_loader= PyPDFDirectoryLoader(directory)\n",
    "    document = file_loader.load()\n",
    "    if not document:\n",
    "            raise ValueError(\"No documents found in the specified directory.\")\n",
    "    return document\n",
    "\n",
    "docs = read_doc(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents we are considering in this tutorial are [research papers](https://papers.nips.cc/paper_files/paper/2023) due to their symmetrical format. Before generating embeddings, we need to split large documents into smaller chunks through a process called text chunking.\n",
    "\n",
    "\n",
    "### Text Chunking {#text-chunking}\n",
    "\n",
    "Text chunking is essential for processing large documents before embedding generation. The process breaks down documents into smaller, manageable pieces that embedding models can process effectively within their token limits.\n",
    "\n",
    "Key concepts for chunking are:\n",
    "\n",
    "- **Chunk size (1000 chars)**: Maximum characters per chunk, ensuring each piece fits within embedding model token limits.\n",
    "- **Chunk overlap (50 chars)**: Characters that overlap between adjacent chunks to prevent important information from being split. (Example: Chunk 1 ends with \"...model performance\" and Chunk 2 starts with \"model performance improves...\")\n",
    "- **Hierarchical separators**: Natural text boundaries like paragraphs (`\\n\\n`), sentences (`.`), and words (` `) that preserve meaning when splitting\n",
    "- **Minimum length filtering (50 chars)**: Removes chunks too short to contain meaningful semantic information. (Example: Filters out headers like \"Introduction\" or \"Figure 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def chunk_data(docs, chunk_size=1000, chunk_overlap=50, min_length=50):\n",
    "    \"\"\"Split documents into smaller chunks for embedding generation.\"\"\"\n",
    "\n",
    "    # Configure text splitter with hierarchical separators\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    # Split documents into chunks\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Filter out headers and short chunks\n",
    "    filtered_chunks = [\n",
    "        chunk for chunk in chunks\n",
    "        if not chunk.page_content.startswith(\"NeurIPS 2023\") and\n",
    "           len(chunk.page_content.strip()) >= min_length\n",
    "    ]\n",
    "\n",
    "    return filtered_chunks\n",
    "\n",
    "# Process documents into chunks\n",
    "chunks = chunk_data(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchical separators `[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]` ensure text breaks at natural boundaries. The splitter prefers paragraph breaks, then line breaks, then sentence endings, maintaining content integrity while creating optimal chunk sizes for semantic search.\n",
    "\n",
    "### Embeddings {#embeddings}\n",
    "\n",
    "The embeddings will be generated in two steps:\n",
    "\n",
    "- Initializing the `OllamaEmbeddings`.\n",
    "- Saving the embeddings in Pinecone\n",
    "\n",
    "#### Python Vector Database with Ollama Embeddings\n",
    "\n",
    "We can simply generate Ollama embeddings using a text model (`mxbai-embed-large` in this case). This approach provides local embedding generation for our Python vector database implementation, ensuring data privacy and reducing API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Initialize Ollama embeddings for Python vector database\n",
    "# Using mxbai-embed-large model for high-quality embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Pinecone Integration Setup\n",
    "\n",
    "Now, we will configure our Pinecone vector database for the LangChain Pinecone integration. First things first, go to the [Pinecone website](https://app.pinecone.io/?sessionType=signup) and make a (free) user account. After making your account, you will be able to get Pinecone's API key for your Python vector database setup. Before setting up the Pinecone vector database, please set up the Pinecone API key in the .env file.\n",
    "\n",
    "The `.env` file will look something like this:\n",
    "\n",
    "```\n",
    "PINECONE_API_KEY=\"your-api-key\"\n",
    "```\n",
    "\n",
    "Pinecone organizes vector data in indexes, with records partitioned into namespaces within each index. We'll create an index named `semantic-search-local` with 1024 dimensions using cosine similarity.\n",
    "\n",
    "The free tier restricts us to the `us-east-1` region, and we'll enable deletion protection to prevent accidental data loss. Here's how to set up the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "index_name = \"semantic-search-local\"\n",
    "namespace = \"langchain-ollama\"\n",
    "\n",
    "# Check if index already exists\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if index_name not in existing_indexes:\n",
    "    print(f\"Creating index '{index_name}'...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "        deletion_protection=\"enabled\"\n",
    "    )\n",
    "\n",
    "    # Wait for index to be ready\n",
    "    print(\"Waiting for index to be ready...\")\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "    print(f\"Index '{index_name}' created and ready.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists. Connecting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if you want to have a quick look at the index, you can do so using the `describe_index_stats()` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to index\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Index 'semantic-search-local' already exists. Connecting...\n",
    "{'dimension': 1024,\n",
    " 'index_fullness': 0.0,\n",
    " 'metric': 'cosine',\n",
    " 'namespaces': {'semantic-search-local-namespace': {'vector_count': 139}},\n",
    " 'total_vector_count': 139,\n",
    " 'vector_type': 'dense'}\n",
    "\n",
    "```\n",
    "\n",
    "And now, we have populated the vector store/db with the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore.from_documents(\n",
    "    documents=chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    "    namespace=index_name + \"-namespace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Semantic Search with Python Vector Database {#implementing-semantic-search-with-python-vector-database}\n",
    "\n",
    "With our Ollama embeddings stored in the Pinecone vector database, we can now perform semantic search using our Python vector database implementation. The process follows three simple steps common to all vector databases.\n",
    "\n",
    "The search methodology involves converting queries to Ollama embeddings, comparing them with stored vectors in the Pinecone vector database, and retrieving the most similar results. We will:\n",
    "\n",
    "- Use `k=2` to get the top 2 matches\n",
    "- Set a similarity threshold of 0.5 to filter out low-quality results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity search\n",
    "# get the data from the database itself (VectorDB)\n",
    "def retrieve_query(query, k=2, score_threshold=0.5):\n",
    "    matching_result = vector_store.similarity_search_with_score(query, k=k)\n",
    "    filtered = [r for r in matching_result if r[1] >= score_threshold]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check some queries and also see the similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What were the key findings of the NeurIPS 2023 LLM Efficiency Fine-tuning Competition?\"\n",
    "x = retrieve_query(query=query, k=1, score_threshold=0.5)\n",
    "for match, score in x:\n",
    "    print(f\"Score: {score:.3f}\")\n",
    "    print(match.page_content[:300])  # Show preview\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will generate an output similar to this:\n",
    "\n",
    "```\n",
    "Score: 0.736\n",
    "for generative models and demonstrate the need for more robust evaluation meth-\n",
    "ods. Notably, the winning submissions utilized standard open-source libraries and\n",
    "focused primarily on data curation. To facilitate further research and promote\n",
    "reproducibility, we release all competition entries, Docker\n",
    "```\n",
    "\n",
    "The similarity score of 0.736 indicates a strong semantic match between our query and the retrieved text. This score exceeds our 0.5 threshold, confirming the result's relevance.\n",
    "\n",
    "Since we set `k=1`, we're retrieving only the single most similar document. This result represents the best semantic match in our entire document collection for the given query.\n",
    "\n",
    "\n",
    "### Measuring Basic Performance Measures {#measuring-basic-performance-measures}\n",
    "\n",
    "Performance monitoring is crucial for production systems, so let's measure our search latency. This gives us baseline metrics for optimization decisions.\n",
    "\n",
    "We'll test with the same research-focused queries from our earlier example. These represent typical academic questions users might ask about NeurIPS papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"What were the key findings of the NeurIPS 2023 LLM Efficiency Fine-tuning Competition?\",\n",
    "    \"How does self-preference bias manifest in LLM evaluators, and what evidence supports this?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search times will vary based on dataset size, network conditions, and query complexity. The measurements below provide a starting point for performance expectations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(queries, k=3):\n",
    "    latencies = []\n",
    "    for query in queries:\n",
    "        start = time.time()\n",
    "        results = retrieve_query(query, k)\n",
    "        duration = time.time() - start\n",
    "        latencies.append(duration)\n",
    "        print(f\"Query: {query[:50]}... | Search Time: {duration:.2f}s\")\n",
    "    avg_latency = sum(latencies) / len(latencies) if latencies else 0\n",
    "    print(f\"Average Search Time: {avg_latency:.2f}s\")\n",
    "    return results\n",
    "\n",
    "measure_latency(test_queries, k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/khuyentran/codecut_content/codecut_articles/.venv/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
