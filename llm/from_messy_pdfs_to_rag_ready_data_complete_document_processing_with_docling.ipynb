{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Document Processing Pipeline\n",
    "\n",
    "### What is Docling?\n",
    "\n",
    "[Docling](https://github.com/DS4SD/docling) is a document parsing library developed by IBM Research. It can convert various document formats (PDF, Word, PowerPoint, Images, HTML) into structured formats suitable for AI applications.\n",
    "\n",
    "What makes Docling special:\n",
    "\n",
    "- **Advanced Layout Understanding**: Uses computer vision models to understand document structure\n",
    "- **Rich Output Formats**: Exports to Markdown, JSON, or Doctags for different use cases  \n",
    "- **OCR Integration**: Handles scanned documents and images\n",
    "- **Table Extraction**: Preserves complex table structures\n",
    "- **Batch Processing**: Efficiently processes multiple documents\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG (Retrieval Augmented Generation) is a technique that combines document retrieval with language generation. Instead of relying solely on training data, RAG systems:\n",
    "\n",
    "1. **Retrieve** relevant documents from a knowledge base\n",
    "2. **Augment** the query with retrieved context  \n",
    "3. **Generate** responses using both the query and retrieved information\n",
    "\n",
    "This approach enables AI systems to work with up-to-date, domain-specific information without retraining the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Initialize converter with default settings\n",
    "converter = DocumentConverter()\n",
    "\n",
    "# Convert any document format - we'll use the Docling technical report itself\n",
    "source_url = \"https://arxiv.org/pdf/2408.09869\"\n",
    "result = converter.convert(source_url)\n",
    "\n",
    "# Access structured data immediately\n",
    "doc = result.document\n",
    "print(f\"Successfully processed document from: {source_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate through each document element, we will use the `doc.iterate_items()` method. This method returns tuples of (item, level). For example:\n",
    "\n",
    "- `(TextItem(label='paragraph', text='Introduction text...'), 0)` - top-level paragraph\n",
    "- `(TableItem(label='table', text='| Col1 | Col2 |...'), 1)` - table at depth 1 \n",
    "- `(TextItem(label='heading', text='Section 2'), 0)` - section heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Create a dictionary to categorize all document elements by type\n",
    "element_types = defaultdict(list)\n",
    "\n",
    "# Iterate through all document elements and group them by label\n",
    "for item, _ in doc.iterate_items():\n",
    "    element_type = item.label\n",
    "    element_types[element_type].append(item)\n",
    "\n",
    "# Display the breakdown of document structure\n",
    "print(\"Document structure breakdown:\")\n",
    "for element_type, items in element_types.items():\n",
    "    print(f\"  {element_type}: {len(items)} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the different types of elements Docling extracted from the document.\n",
    "\n",
    "```text\n",
    "Document structure breakdown:\n",
    "  picture: 13 elements\n",
    "  section_header: 31 elements\n",
    "  text: 102 elements\n",
    "  list_item: 22 elements\n",
    "  code: 2 elements\n",
    "  footnote: 1 elements\n",
    "  caption: 3 elements\n",
    "  table: 5 elements\n",
    "```\n",
    "\n",
    "Let's look specifically for structured elements like tables and formulas that are crucial for RAG applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_table = element_types[\"table\"][0]\n",
    "print(first_table.export_to_dataframe().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    | CPU.                        | Thread budget.   | native backend.TTS   | native backend.Pages/s   | native backend.Mem   | pypdfium backend.TTS   | pypdfium backend.Pages/s   | pypdfium backend.Mem   |\n",
    "|---:|:----------------------------|:-----------------|:---------------------|:-------------------------|:---------------------|:-----------------------|:---------------------------|:-----------------------|\n",
    "|  0 | Apple M3 Max                | 4                | 177 s 167 s          | 1.27 1.34                | 6.20 GB              | 103 s 92 s             | 2.18 2.45                  | 2.56 GB                |\n",
    "|  1 | (16 cores) Intel(R) E5-2690 | 16 4 16          | 375 s 244 s          | 0.60 0.92                | 6.16 GB              | 239 s 143 s            | 0.94 1.57                  | 2.42 GB                |\n",
    "\n",
    "Here is how the table looks in the original PDF:\n",
    "\n",
    "![CPU performance comparison table from the original PDF document showing merged cells structure](https://codecut.ai/wp-content/uploads/2025/07/table.png)\n",
    "\n",
    "The extracted table shows Docling's accuracy and structural differences from the original PDF. Docling captured all numerical data and text perfectly but flattened the merged cell structure into separate columns.\n",
    "\n",
    "While this loses visual formatting, it benefits RAG applications since each row contains complete information without complex cell merging logic.\n",
    "\n",
    "Next, look at the first list item element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_list_items = element_types[\"list_item\"][0:6]\n",
    "for list_item in first_list_items:\n",
    "    print(list_item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "· Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
    "· Understands detailed page layout, reading order, locates figures and recovers table structures\n",
    "· Extracts metadata from the document, such as title, authors, references and language\n",
    "· Optionally applies OCR, e.g. for scanned PDFs\n",
    "· Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
    "· Can leverage different accelerators (GPU, MPS, etc).\n",
    "```\n",
    "\n",
    "This matches the original PDF list item.\n",
    "\n",
    "![Original PDF showing the first 6 list items](https://codecut.ai/wp-content/uploads/2025/07/list.png)\n",
    "\n",
    "Look at the first caption element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_caption = element_types[\"caption\"][0]\n",
    "print(first_caption.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches the image caption in the original PDF.\n",
    "\n",
    "![Screenshot of a technical diagram from the original PDF showing Docling's processing pipeline with labeled components and arrows indicating workflow sequence](https://codecut.ai/wp-content/uploads/2025/07/image-with-caption.png)\n",
    "\n",
    "This matches the image caption in the original PDF.\n",
    "\n",
    "## Export Options for Different Use Cases {#export-options-for-different-use-cases}\n",
    "\n",
    "Docling provides multiple ways to export the document data, including Markdown, JSON, and dictionary formats.\n",
    "\n",
    "For human review and documentation, Markdown format preserves the document structure beautifully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human-readable markdown for review\n",
    "markdown_content = doc.export_to_markdown()\n",
    "print(markdown_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "<!-- image -->\n",
    "\n",
    "## Docling Technical Report\n",
    "\n",
    "Version 1.0\n",
    "\n",
    "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\n",
    "\n",
    "AI4K Group, IBM Research R¨ uschlikon, Switzerland\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This technical report introduces Docling , an easy to use, self-contained, MITli...\n",
    "```\n",
    "\n",
    "Compare this to the original PDF:\n",
    "\n",
    "![Screenshot of the original PDF title page showing \"Docling Technical Report Version 1.0\" with multiple author names and IBM Research affiliation in formal academic layout](https://codecut.ai/wp-content/uploads/2025/07/title-page.png)\n",
    "\n",
    "Docling preserves all original content while converting complex PDF formatting into clean markdown. Every author name, title, and abstract text remains intact, creating searchable structure perfect for RAG applications.\n",
    "\n",
    "For programmatic processing and API integrations, JSON format provides structured access to all document elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# JSON for programmatic processing\n",
    "json_dict = doc.export_to_dict()\n",
    "\n",
    "print(\"JSON keys:\", json_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "JSON keys: dict_keys(['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'form_items', 'pages'])\n",
    "```\n",
    "\n",
    "The JSON structure reveals Docling's comprehensive document analysis. Key sections include `texts` for paragraphs, `tables` for structured data, `pictures` for images, and `pages` for layout information.\n",
    "\n",
    "For Python development workflows, the dictionary format enables immediate access to all document elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python dictionary for immediate use\n",
    "dict_repr = doc.export_to_dict()\n",
    "\n",
    "# Preview the structure\n",
    "num_texts = len(dict_repr[\"texts\"])\n",
    "num_tables = len(dict_repr[\"tables\"])\n",
    "\n",
    "print(f\"Text elements: {num_texts}\")\n",
    "print(f\"Table elements: {num_tables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Text elements: 985\n",
    "Table elements: 5\n",
    "```\n",
    "\n",
    "## Configuring PdfPipelineOptions for Advanced Processing {#configuring-pdfpipelineoptions-for-advanced-processing}\n",
    "\n",
    "The default Docling configuration works well for most documents, but `PdfPipelineOptions` unlocks advanced processing capabilities. These options control OCR engines, table recognition, AI enrichments, and performance settings.\n",
    "\n",
    "`PdfPipelineOptions` becomes essential when working with scanned documents, complex layouts, or specialized content requiring AI-powered understanding.\n",
    "\n",
    "### Enable Image Extraction {#enable-image-extraction}\n",
    "\n",
    "By default, Docling does not extract images from the document. However, you can enable image extraction by setting the `generate_picture_images` option to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import PdfFormatOption\n",
    "\n",
    "pipeline_options = PdfPipelineOptions(generate_picture_images=True)\n",
    "\n",
    "# Create converter with enhanced table processing\n",
    "converter_enhanced = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "result_enhanced = converter_enhanced.convert(\"https://arxiv.org/pdf/2408.09869\")\n",
    "doc_enhanced = result_enhanced.document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display the first image\n",
    "from IPython.display import Image, display\n",
    "\n",
    "for item, _ in doc_enhanced.iterate_items():\n",
    "    if item.label == \"picture\":\n",
    "        image_data = item.image\n",
    "\n",
    "        # Get the image URI\n",
    "        uri = str(image_data.uri)\n",
    "\n",
    "        # Display the image using IPython\n",
    "        display(Image(url=uri))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Yellow rubber duck toy on light surface](https://codecut.ai/wp-content/uploads/2025/07/duck-image.png)\n",
    "\n",
    "The output image matches the first image of the PDF.\n",
    "\n",
    "\n",
    "### Table Recognition Enhancement {#table-recognition-enhancement}\n",
    "\n",
    "To use the more sophisticated AI model for table extraction instead of the default fast model, you can set the `table_structure_options.mode` to `TableFormerMode.ACCURATE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode\n",
    "from docling.document_converter import PdfFormatOption\n",
    "\n",
    "# Enhanced table processing for complex layouts\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE\n",
    "\n",
    "# Create converter with enhanced table processing\n",
    "converter_enhanced = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "result_enhanced = converter_enhanced.convert(\"https://arxiv.org/pdf/2408.09869\")\n",
    "doc_enhanced = result_enhanced.document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI-Powered Content Understanding {#ai-powered-content-understanding}\n",
    "\n",
    "AI enrichments enhance extracted content with semantic understanding. Picture descriptions, formula detection, and code parsing improve RAG accuracy by adding crucial context.\n",
    "\n",
    "In the code below, we:\n",
    "\n",
    "- Set the `do_picture_description` option to `True` to enable picture description extraction\n",
    "- Set the `picture_description_options` option to use the `SmolVLM-256M-Instruct` model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.pipeline_options import PictureDescriptionVlmOptions\n",
    "\n",
    "# AI-powered content enrichment\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    do_picture_description=True,  # AI-generated image descriptions\n",
    "    picture_description_options=PictureDescriptionVlmOptions(\n",
    "        repo_id=\"HuggingFaceTB/SmolVLM-256M-Instruct\",\n",
    "        prompt=\"Describe this picture. Be precise and concise.\",\n",
    "    ),\n",
    "    generate_picture_images=True,\n",
    ")\n",
    "\n",
    "converter_enhanced = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "result_enhanced = converter_enhanced.convert(\"https://arxiv.org/pdf/2408.09869\")\n",
    "doc_enhanced = result_enhanced.document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the picture description from the second picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_picture = doc_enhanced.pictures[1]\n",
    "\n",
    "print(f\"Caption: {second_picture.caption_text(doc=doc_enhanced)}\")\n",
    "\n",
    "# Check for annotations\n",
    "for annotation in second_picture.annotations:\n",
    "    print(annotation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Caption: Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\n",
    "### Image Description\n",
    "\n",
    "The image is a flowchart that depicts a sequence of steps from a document, likely a report or a document. The flowchart is structured with various elements such as text, icons, and arrows. Here is a detailed description of the flowchart:\n",
    "\n",
    "#### Step 1: Parse\n",
    "- **Description:** The first step in the process is to parse the document. This involves converting the text into a format that can be easily understood by the user.\n",
    "\n",
    "#### Step 2: Ocr\n",
    "- **Description:** The second step is to perform OCR (Optical Character Recognition) on the document. This involves converting the text into a format that can be easily read by the OCR software.\n",
    "\n",
    "#### Step 3: Layout Analysis\n",
    "- **Description:** The third step is to analyze the document's layout. This involves examining the document's structure, including the layout of the text, the alignment of the text, and the alignment of the document's content\n",
    "```\n",
    "\n",
    "Here is the original image:\n",
    "\n",
    "![Technical flowchart diagram from original PDF showing Docling's processing pipeline with parse, OCR, and layout analysis steps](https://codecut.ai/wp-content/uploads/2025/07/image-with-caption.png)\n",
    "\n",
    "The detailed description shows how Docling's picture analysis transforms visual content into text that can be indexed and searched, making diagrams accessible to RAG systems.\n",
    "\n",
    "\n",
    "### Performance and Memory Management {#performance-and-memory-management}\n",
    "\n",
    "Processing a large document can be time-consuming. To speed up the process, we can use:\n",
    "\n",
    "- The `page_range` option to process only a specific page range.\n",
    "- The `max_num_pages` option to limit the number of pages to process.\n",
    "- The `images_scale` option to reduce the image resolution for speed.\n",
    "- The `generate_page_images` option to skip page images to save memory.\n",
    "- The `do_table_structure` option to skip table structure extraction.\n",
    "- The `enable_parallel_processing` option to use multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized for large documents\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    max_num_pages=4,  # Limit processing to first 4 pages\n",
    "    page_range=[1, 3],  # Process specific page range\n",
    "    generate_page_images=False,  # Skip page images to save memory\n",
    "    do_table_structure=False,  # Skip table structure extraction\n",
    "    enable_parallel_processing=True  # Use multiple cores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Your RAG Pipeline {#building-your-rag-pipeline}\n",
    "\n",
    "We'll build our RAG pipeline in five steps:\n",
    "\n",
    "- **Document Processing**: Use Docling to convert documents into structured data\n",
    "- **Chunking**: Break documents into smaller, searchable pieces\n",
    "- **Create Embeddings**: Convert text chunks into vector representations\n",
    "- **Store in Vector Database**: Save embeddings in FAISS for fast similarity search\n",
    "- **Query**: Retrieve relevant chunks and generate contextual responses\n",
    "\n",
    "### Tools for RAG Pipelines {#tools-for-rag-pipelines}\n",
    "\n",
    "Building RAG pipelines requires four essential tools:\n",
    "\n",
    "- Docling: converts documents into structured data\n",
    "- LangChain: manages document workflows, chain orchestration, and provides embedding models\n",
    "- FAISS: stores and retrieves document chunks\n",
    "\n",
    "These tools work together to create complete RAG pipelines that can process, store, and retrieve document content intelligently.\n",
    "\n",
    "\n",
    "#### LangChain\n",
    "\n",
    "[LangChain](https://github.com/langchain-ai/langchain) simplifies building AI applications by providing components for document loading, text processing, and chain orchestration. It integrates seamlessly with vector stores and language models.\n",
    "\n",
    "For a comprehensive introduction to LangChain fundamentals and local AI workflows, see our [LangChain and Ollama guide](https://codecut.ai/private-ai-workflows-langchain-ollama/).\n",
    "\n",
    "#### FAISS\n",
    "\n",
    "[FAISS](https://github.com/facebookresearch/faiss) (Facebook AI Similarity Search) is a library for efficient similarity search in high-dimensional spaces. It enables fast retrieval of the most relevant document chunks based on embedding similarity.\n",
    "\n",
    "For production use cases requiring robust database integration, consider [implementing semantic search with pgvector in PostgreSQL](https://codecut.ai/semantic-search-postgres-pgvector-ollama/) or [using Pinecone for cloud-based vector search](https://codecut.ai/pinecone-ollama-semantic-search-tutori/) as alternatives to FAISS.\n",
    "\n",
    "Let's install the additional packages for RAG functionality:\n",
    "\n",
    "```bash\n",
    "# Install additional packages for RAG functionality\n",
    "pip install docling sentence-transformers langchain-community langchain-huggingface faiss-cpu\n",
    "# Note: Use faiss-gpu if you have CUDA support\n",
    "```\n",
    "\n",
    "### Document Processing {#document-processing}\n",
    "\n",
    "Convert the document into structured data using Docling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Initialize converter with default settings\n",
    "converter = DocumentConverter()\n",
    "\n",
    "# Convert the document into structured data\n",
    "source_url = \"https://arxiv.org/pdf/2408.09869\"\n",
    "result = converter.convert(source_url)\n",
    "\n",
    "# Access structured data immediately\n",
    "doc = result.document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking {#chunking}\n",
    "\n",
    "AI models have limited context windows that can't process entire documents at once. Chunking solves this by breaking documents into smaller, searchable pieces that fit within these constraints. This improves retrieval accuracy by finding the most relevant sections rather than entire documents.\n",
    "\n",
    "Docling provides two main chunking strategies:\n",
    "\n",
    "- **HierarchicalChunker**: Focuses purely on document structure, creating chunks based on headings and sections\n",
    "- **HybridChunker**: Combines structure-aware chunking with token-based limits, preserving document hierarchy while respecting model constraints\n",
    "\n",
    "Let's compare how these chunkers process the same document.\n",
    "\n",
    "First, create a helper function to print the chunk content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chunk(chunk):\n",
    "    print(f\"Chunk length: {len(chunk.text)} characters\")\n",
    "    if len(chunk.text) > 30:\n",
    "        print(f\"Chunk content: {chunk.text[:30]}...{chunk.text[-30:]}\")\n",
    "    else:\n",
    "        print(f\"Chunk content: {chunk.text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, process the document with the `HierarchicalChunker`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HierarchicalChunker\n",
    "\n",
    "# Process with HierarchicalChunker (structure-based)\n",
    "hierarchical_chunker = HierarchicalChunker()\n",
    "hierarchical_chunks = list(hierarchical_chunker.chunk(doc))\n",
    "\n",
    "print(f\"HierarchicalChunker: {len(hierarchical_chunks)} chunks\")\n",
    "\n",
    "# Print the first 3 chunks\n",
    "for chunk in hierarchical_chunks[:5]:\n",
    "    print_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "HierarchicalChunker: 114 chunks\n",
    "Chunk length: 11 characters\n",
    "Chunk content: Version 1.0\n",
    "--------------------------------------------------\n",
    "Chunk length: 295 characters\n",
    "Chunk content: Christoph Auer Maksym Lysak Ah... Kuropiatnyk Peter W. J. Staar\n",
    "--------------------------------------------------\n",
    "Chunk length: 50 characters\n",
    "Chunk content: AI4K Group, IBM Research R¨ us...arch R¨ uschlikon, Switzerland\n",
    "--------------------------------------------------\n",
    "Chunk length: 431 characters\n",
    "Chunk content: This technical report introduc...on of new features and models.\n",
    "--------------------------------------------------\n",
    "Chunk length: 792 characters\n",
    "Chunk content: Converting PDF documents back ... gap to proprietary solutions.\n",
    "--------------------------------------------------\n",
    "```\n",
    "\n",
    "Compare this to the `HybridChunker`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "\n",
    "# Process with HybridChunker (token-aware)\n",
    "hybrid_chunker = HybridChunker(max_tokens=512, overlap_tokens=50)\n",
    "hybrid_chunks = list(hybrid_chunker.chunk(doc))\n",
    "\n",
    "print(f\"HybridChunker: {len(hybrid_chunks)} chunks\")\n",
    "\n",
    "# Print the first 3 chunks\n",
    "for chunk in hybrid_chunks[:5]:\n",
    "    print_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "HybridChunker: 50 chunks\n",
    "Chunk length: 358 characters\n",
    "Chunk content: Version 1.0\n",
    "Christoph Auer Mak...arch R¨ uschlikon, Switzerland\n",
    "--------------------------------------------------\n",
    "Chunk length: 431 characters\n",
    "Chunk content: This technical report introduc...on of new features and models.\n",
    "--------------------------------------------------\n",
    "Chunk length: 1858 characters\n",
    "Chunk content: Converting PDF documents back ... accelerators (GPU, MPS, etc).\n",
    "--------------------------------------------------\n",
    "Chunk length: 1436 characters\n",
    "Chunk content: To use Docling, you can simply...and run it inside a container.\n",
    "--------------------------------------------------\n",
    "Chunk length: 796 characters\n",
    "Chunk content: Docling implements a linear pi...erialized to JSON or Markdown.\n",
    "--------------------------------------------------\n",
    "```\n",
    "\n",
    "The comparison shows key differences:\n",
    "\n",
    "- **HierarchicalChunker**: Creates many small chunks by splitting at every section boundary\n",
    "- **HybridChunker**: Creates fewer, larger chunks by combining related sections within token limits\n",
    "\n",
    "\n",
    "We will use `HybridChunker` because it respects document boundaries (won't split tables inappropriately) while ensuring chunks fit within embedding model constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "\n",
    "# Initialize the chunker\n",
    "chunker = HybridChunker(max_tokens=512, overlap_tokens=50)\n",
    "\n",
    "# Create the chunks\n",
    "rag_chunks = list(chunker.chunk(doc))\n",
    "\n",
    "print(f\"Created {len(rag_chunks)} intelligent chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Created 50 intelligent chunks\n",
    "```\n",
    "\n",
    "### Creating a Vector Store {#creating-a-vector-store}\n",
    "\n",
    "A vector store is a database that converts text into numerical vectors called embeddings. These vectors capture semantic meaning, allowing the system to find related content even when different words are used.\n",
    "\n",
    "When you search for \"document processing,\" the vector store finds chunks about \"PDF parsing\" or \"text extraction\" because their embeddings are mathematically close. This enables semantic search beyond exact keyword matching.\n",
    "\n",
    "Create the vector store for semantic search across your document chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the vector store\n",
    "texts = [chunk.text for chunk in rag_chunks]\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "print(f\"Built vector store with {len(texts)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Built vector store with 50 chunks\n",
    "```\n",
    "\n",
    "Now you can search your knowledge base with semantic similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the knowledge base\n",
    "query = \"How does document processing work?\"\n",
    "relevant_docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Found {len(relevant_docs)} relevant chunks:\")\n",
    "\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Content: {doc.page_content[:150]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/khuyentran/codecut_content/codecut_articles/.venv/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
