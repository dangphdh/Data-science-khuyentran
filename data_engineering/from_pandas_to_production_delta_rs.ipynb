{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake: Transform pandas Prototypes into Production\n",
    "\n",
    "## Setup and Data Preparation\n",
    "\n",
    "Install Delta-rs and supporting libraries:\n",
    "\n",
    "```bash\n",
    "pip install deltalake pandas duckdb polars\n",
    "```\n",
    "\n",
    "We'll use actual NYC Yellow Taxi data to demonstrate real-world scenarios. The NYC Taxi & Limousine Commission provides monthly trip records in Parquet format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from deltalake import DeltaTable, write_deltalake\n",
    "\n",
    "# Download NYC Yellow Taxi data (June 2024 as example)\n",
    "# Full dataset available at: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "taxi_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet\"\n",
    "\n",
    "# Read a sample of the data for demonstration\n",
    "sample_data = pd.read_parquet(taxi_url).head(10000)\n",
    "\n",
    "print(f\"Loaded {len(sample_data)} taxi trips from NYC TLC\")\n",
    "print(f\"Data shape: {sample_data.shape}\")\n",
    "print(f\"Date range: {sample_data['tpep_pickup_datetime'].min()} to {sample_data['tpep_pickup_datetime'].max()}\")\n",
    "\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your First Delta Table\n",
    "\n",
    "Create your first Delta table in the `data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_deltalake(\"data/taxi_delta_table\", sample_data, mode=\"overwrite\")\n",
    "print(\"Created Delta table\")\n",
    "\n",
    "# Read back from Delta table\n",
    "dt = DeltaTable(\"data/taxi_delta_table\")\n",
    "df_from_delta = dt.to_pandas()\n",
    "\n",
    "print(f\"Delta table contains {len(df_from_delta)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the Delta table structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Delta table metadata\n",
    "print(\"Delta table schema:\")\n",
    "print(dt.schema().to_arrow())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the current version of the Delta table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current version: {dt.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Updates and CRUD Operations\n",
    "\n",
    "Production pipelines require efficient handling of late-arriving data and targeted updates. Delta-rs enables surgical data operations without full dataset rewrites.\n",
    "\n",
    "To demonstrate this, we'll simulate late-arriving data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate late-arriving data\n",
    "late_data = pd.read_parquet(taxi_url).iloc[10000:10050]\n",
    "print(f\"New data to add: {len(late_data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Approach: Process Everything\n",
    "\n",
    "The pandas workflow requires loading both existing and new data, combining them, and rewriting the entire output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas approach - reload existing data and merge\n",
    "existing_df = pd.read_parquet(taxi_url).head(10000)\n",
    "complete_df = pd.concat([existing_df, late_data])\n",
    "complete_df.to_parquet(\"data/taxi_complete.parquet\")\n",
    "print(f\"Processed {len(complete_df)} total records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta-rs Approach: Process Only New Data\n",
    "\n",
    "Delta-rs performs surgical operations, appending only the new records without touching existing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta-rs - append only what's new\n",
    "write_deltalake(\"data/taxi_delta_table\", late_data, mode=\"append\")\n",
    "\n",
    "dt = DeltaTable(\"data/taxi_delta_table\")\n",
    "print(f\"Added {len(late_data)} new records\")\n",
    "print(f\"Table version: {dt.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel and Data Versioning\n",
    "\n",
    "Production systems need reliable access to historical data for audits and rollbacks. Delta-rs provides automatic versioning without the storage overhead and complexity of manual backup strategies.\n",
    "\n",
    "### Traditional Approach: Manual Backup Strategy\n",
    "\n",
    "Traditional file-based workflows rely on timestamped copies and manual versioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional approach - manual timestamped backups\n",
    "import os\n",
    "\n",
    "# Create a backup directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Example of manual backup workflow (commented out to avoid overwriting)\n",
    "# timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# sample_data.to_parquet(f\"data/taxi_backup_{timestamp}.parquet\")  # Create manual backup\n",
    "# sample_data.to_parquet(\"data/taxi_data.parquet\")  # Overwrite original\n",
    "\n",
    "print(\"Traditional approach requires manual backup creation and file management\")\n",
    "print(\"To recover data, you would need to manually identify and reload backup files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta-rs Approach: Built-in Time Travel\n",
    "\n",
    "Delta-rs automatically tracks every change with instant access to any version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access any historical version instantly\n",
    "dt_v0 = DeltaTable(\"data/taxi_delta_table\", version=0)\n",
    "current_dt = DeltaTable(\"data/taxi_delta_table\")\n",
    "\n",
    "print(f\"Version 0: {len(dt_v0.to_pandas())} records\")\n",
    "print(f\"Current version: {len(current_dt.to_pandas())} records\")\n",
    "print(f\"Available versions: {current_dt.version() + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Evolution in Action\n",
    "\n",
    "Data structures evolve as business requirements change. Delta-rs automatically handles schema changes without breaking existing pipelines or requiring migration scripts.\n",
    "\n",
    "To demonstrate this, imagine NYC's taxi authority introduces weather tracking and surge pricing features, requiring your pipeline to handle new `weather_condition` and `surge_multiplier` columns alongside existing fare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the existing data\n",
    "enhanced_data = pd.read_parquet(taxi_url).iloc[20000:20100].copy()\n",
    "\n",
    "# Simulate new data with additional business columns\n",
    "weather_options = [\"clear\", \"rain\", \"snow\", \"cloudy\"]\n",
    "surge_options = [1.0, 1.2, 1.5, 2.0]\n",
    "enhanced_data[\"weather_condition\"] = [weather_options[i % 4] for i in range(len(enhanced_data))]\n",
    "enhanced_data[\"surge_multiplier\"] = [surge_options[i % 4] for i in range(len(enhanced_data))]\n",
    "\n",
    "print(f\"Enhanced data: {len(enhanced_data)} records with {len(enhanced_data.columns)} columns\")\n",
    "print(f\"New columns: {[col for col in enhanced_data.columns if col not in sample_data.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Approach: No Schema History\n",
    "\n",
    "Traditional formats provide no tracking of schema changes or evolution history:\n",
    "\n",
    "```python\n",
    "# Traditional approach - no schema versioning or history\n",
    "df_v1 = pd.read_parquet(\"taxi_v1.parquet\")  # Original schema\n",
    "df_v2 = pd.read_parquet(\"taxi_v2.parquet\")  # Enhanced schema\n",
    "```\n",
    "\n",
    "### Delta-rs Approach: Schema Versioning and History\n",
    "\n",
    "Delta-rs automatically merges schemas while tracking every change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema evolution with automatic versioning\n",
    "write_deltalake(\n",
    "    \"data/taxi_delta_table\", \n",
    "    enhanced_data, \n",
    "    mode=\"append\",\n",
    "    schema_mode=\"merge\"\n",
    ")\n",
    "\n",
    "dt = DeltaTable(\"data/taxi_delta_table\")\n",
    "print(f\"Schema evolved: {len(dt.to_pandas().columns)} columns | Version: {dt.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the complete schema evolution history and access any previous version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View schema change history\n",
    "history = dt.history()\n",
    "for entry in history[:2]:\n",
    "    print(f\"Version {entry['version']}: {entry['operation']} at {entry['timestamp']}\")\n",
    "\n",
    "# Access different schema versions\n",
    "original_schema = DeltaTable(\"data/taxi_delta_table\", version=0)\n",
    "print(f\"\\nOriginal schema (v0): {len(original_schema.to_pandas().columns)} columns\")\n",
    "print(f\"Current schema (v{dt.version()}): {len(dt.to_pandas().columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Engine Integration\n",
    "\n",
    "Cross-functional teams require unified data access across different analytical tools. Delta-rs eliminates format conversion overhead with native multi-engine support.\n",
    "\n",
    "### Traditional Approach: Engine-Specific Optimization Requirements\n",
    "\n",
    "Each engine needs different file optimizations that don't transfer between tools.\n",
    "\n",
    "Start with the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional approach - Each engine needs different optimizations\n",
    "data = {\"payment_type\": [1, 1, 2, 1, 2], \"fare_amount\": [15.5, 20.0, 18.3, 12.5, 25.0]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas team optimizes for indexed lookups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas team needs indexed Parquet for fast lookups\n",
    "df.to_parquet(\"data/pandas_optimized.parquet\", index=True)\n",
    "pandas_result = pd.read_parquet(\"data/pandas_optimized.parquet\")\n",
    "print(f\"Pandas: {len(pandas_result)} trips, avg ${pandas_result['fare_amount'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Polars team needs sorted data for predicate pushdown optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars team needs sorted columns for predicate pushdown\n",
    "df.sort_values(\"payment_type\").to_parquet(\"data/polars_optimized.parquet\")\n",
    "polars_result = pl.read_parquet(\"data/polars_optimized.parquet\").select([\n",
    "    pl.len().alias(\"trips\"), pl.col(\"fare_amount\").mean().alias(\"avg_fare\")\n",
    "])\n",
    "print(f\"Polars: {polars_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DuckDB team requires specific compression for query performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDB needs specific compression/statistics for query planning\n",
    "df.to_parquet(\"data/duckdb_optimized.parquet\", compression=\"zstd\")\n",
    "duckdb_result = duckdb.execute(\"\"\"\n",
    "    SELECT COUNT(*) as trips, ROUND(AVG(fare_amount), 2) as avg_fare\n",
    "    FROM 'data/duckdb_optimized.parquet'\n",
    "\"\"\").fetchone()\n",
    "print(f\"DuckDB: {duckdb_result[0]} trips, ${duckdb_result[1]} avg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta-rs Approach: Universal Optimizations\n",
    "\n",
    "Delta-rs provides built-in optimizations that benefit all engines simultaneously.\n",
    "\n",
    "Create one optimized Delta table that serves all engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta-rs approach - Universal optimizations for all engines\n",
    "import duckdb\n",
    "import polars as pl\n",
    "from deltalake import DeltaTable, write_deltalake\n",
    "\n",
    "# Create Delta table with built-in optimizations:\n",
    "data = {\"payment_type\": [1, 1, 2, 1, 2], \"fare_amount\": [15.5, 20.0, 18.3, 12.5, 25.0]}\n",
    "write_deltalake(\"data/universal_demo\", pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas benefits from Delta's statistics for efficient filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas gets automatic optimization benefits\n",
    "dt = DeltaTable(\"data/universal_demo\")\n",
    "pandas_result = dt.to_pandas()\n",
    "print(f\"Pandas: {len(pandas_result)} trips, avg ${pandas_result['fare_amount'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars leverages Delta's column statistics for predicate pushdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars gets predicate pushdown optimization automatically\n",
    "polars_result = pl.read_delta(\"data/universal_demo\").select([\n",
    "    pl.len().alias(\"trips\"), \n",
    "    pl.col(\"fare_amount\").mean().alias(\"avg_fare\")\n",
    "])\n",
    "print(f\"Polars: {polars_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DuckDB uses Delta's statistics for query planning optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDB gets optimized query plans from Delta statistics\n",
    "duckdb_result = duckdb.execute(\"\"\"\n",
    "    SELECT COUNT(*) as trips, ROUND(AVG(fare_amount), 2) as avg_fare\n",
    "    FROM delta_scan('data/universal_demo')\n",
    "\"\"\").fetchone()\n",
    "print(f\"DuckDB: {duckdb_result[0]} trips, ${duckdb_result[1]} avg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization\n",
    "\n",
    "Production systems require efficient storage management and query performance. Delta-rs provides built-in optimization without manual cleanup scripts or downtime.\n",
    "\n",
    "### Traditional Approach: Manual Cleanup Scripts\n",
    "\n",
    "Traditional workflows require custom scripts to manage file cleanup:\n",
    "\n",
    "```python\n",
    "# Traditional approach - manual file management\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Find old backup files manually\n",
    "old_files = []\n",
    "cutoff_date = datetime.now() - timedelta(days=7)\n",
    "for file in glob.glob(\"data/taxi_backup_*.parquet\"):\n",
    "    file_time = datetime.fromtimestamp(os.path.getmtime(file))\n",
    "    if file_time < cutoff_date:\n",
    "        old_files.append(file)\n",
    "        os.remove(file)  # Manual cleanup with risk\n",
    "```\n",
    "\n",
    "### Delta-rs Approach: Built-in Vacuum Operation\n",
    "\n",
    "Delta-rs provides safe, automated cleanup through its `vacuum()` operation, which removes unused transaction files while preserving data integrity. Files become unused when:\n",
    "\n",
    "• **UPDATE operations** create new versions, leaving old data files unreferenced\n",
    "• **DELETE operations** remove data, making those files obsolete  \n",
    "• **Failed transactions** leave temporary files that were never committed\n",
    "• **Table optimization** consolidates small files, making originals unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta-rs vacuum removes unused files safely with ACID protection\n",
    "import os\n",
    "\n",
    "from deltalake import DeltaTable\n",
    "\n",
    "dt = DeltaTable(\"data/taxi_delta_table\")\n",
    "\n",
    "def get_size(path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            total_size += os.path.getsize(os.path.join(dirpath, filename))\n",
    "    return total_size / (1024 * 1024)\n",
    "\n",
    "# Delta-rs automatically protects against concurrent operations\n",
    "before_size = get_size(\"data/taxi_delta_table\")\n",
    "\n",
    "# Safe cleanup - files only deleted if no active readers/writers\n",
    "dt.vacuum(retention_hours=168)  # Built-in safety: won't delete files in use\n",
    "\n",
    "after_size = get_size(\"data/taxi_delta_table\")\n",
    "\n",
    "print(\"Delta vacuum completed safely\")\n",
    "print(f\"Storage before: {before_size:.1f} MB\")\n",
    "print(f\"Storage after: {after_size:.1f} MB\")\n",
    "print(f\"Space reclaimed: {before_size - after_size:.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/khuyentran/codecut_content/codecut_articles/.venv/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
