{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is DuckDB?\n",
    "\n",
    "[DuckDB](https://github.com/duckdb/duckdb) is a fast, in-process SQL OLAP database optimized for analytics. DuckDB is a great tool for data scientists because of the following reasons:\n",
    "\n",
    "1. **Zero Configuration**: No need to set up or maintain a separate database server\n",
    "2. **Memory Efficiency**: Process large datasets without loading everything into memory\n",
    "3. **Familiar Interface**: Use SQL syntax you already know, directly in Python\n",
    "4. **Performance**: Faster than pandas for many operations, especially joins and aggregations\n",
    "5. **File Format Support**: Direct querying of CSV, Parquet, and other file formats\n",
    "\n",
    "To install DuckDB, run the following command:\n",
    "\n",
    "```bash\n",
    "pip install duckdb\n",
    "```\n",
    "\n",
    "In the next few sections, we'll dive into some of the key features of DuckDB and how it can be used to supercharge your data science workflow.\n",
    "\n",
    "## Zero Configuration\n",
    "\n",
    "SQL operations on DataFrames typically require setting up and maintaining separate database servers, adding complexity to analytical workflows.\n",
    "\n",
    "For example, to perform a simple SQL operation on a DataFrame, you need to:\n",
    "\n",
    "- Install and configure a database server (like PostgreSQL or MySQL)\n",
    "- Ensure the database service is running before executing queries\n",
    "- Set up database credentials\n",
    "- Create a connection to the database\n",
    "- Write the DataFrame to a PostgreSQL table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create sample data\n",
    "sales = pd.DataFrame(\n",
    "    {\n",
    "        \"product\": [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"] * 2,\n",
    "        \"region\": [\"North\", \"South\"] * 6,\n",
    "        \"amount\": [100, 150, 200, 120, 180, 220, 110, 160, 210, 130, 170, 230],\n",
    "        \"date\": pd.date_range(\"2024-01-01\", periods=12),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Create a connection to PostgreSQL\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/postgres\")\n",
    "\n",
    "# Write the DataFrame to a PostgreSQL table\n",
    "sales.to_sql(\"sales\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Execute SQL query against the PostgreSQL database\n",
    "with engine.connect() as conn:\n",
    "    result = pd.read_sql(\"SELECT product, region, amount FROM sales\", conn)\n",
    "\n",
    "print(result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "  product region  amount\n",
    "0       A  North     100\n",
    "1       B  South     150\n",
    "2       C  North     200\n",
    "3       A  South     120\n",
    "4       B  North     180\n",
    "```\n",
    "\n",
    "This overhead can be particularly cumbersome when you just want to perform quick SQL operations on your data.\n",
    "\n",
    "DuckDB simplifies this process by providing direct SQL operations on DataFrames without server setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Direct SQL operations on DataFrame - no server needed\n",
    "result = duckdb.sql(\"SELECT product, region, amount FROM sales\").df()\n",
    "\n",
    "print(result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "product region  amount\n",
    "0       A  North     100\n",
    "1       B  South     150\n",
    "2       C  North     200\n",
    "3       A  South     120\n",
    "4       B  North     180\n",
    "```\n",
    "\n",
    "## Integrate Seamlessly with pandas and Polars {#integrate-seamlessly-with-pandas-and-polars}\n",
    "\n",
    "Have you ever wanted to leverage SQL's power while working with your favorite data manipulation libraries such as pandas and Polars?\n",
    "\n",
    "DuckDB makes it seamless to query pandas and Polars DataFrames via the `duckdb.sql` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "pd_df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n",
    "\n",
    "pl_df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n",
    "\n",
    "duckdb.sql(\"SELECT * FROM pd_df\").df()\n",
    "duckdb.sql(\"SELECT * FROM pl_df\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DuckDB's integration with pandas and Polars lets you combine the strengths of each tool. For example, you can:\n",
    "\n",
    "- Use pandas for data cleaning and feature engineering\n",
    "- Use DuckDB for complex aggregations and complex queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Use pandas for data cleaning and feature engineering\n",
    "sales[\"month\"] = sales[\"date\"].dt.month\n",
    "sales[\"is_high_value\"] = sales[\"amount\"] > 150\n",
    "print(\"Sales after feature engineering:\")\n",
    "print(sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DuckDB for complex aggregations\n",
    "analysis = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        product,\n",
    "        region,\n",
    "        COUNT(*) as total_sales,\n",
    "        AVG(amount) as avg_amount,\n",
    "        SUM(CASE WHEN is_high_value THEN 1 ELSE 0 END) as high_value_sales\n",
    "    FROM sales\n",
    "    GROUP BY product, region\n",
    "    ORDER BY avg_amount DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Sales analysis by product and region:\")\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas for visualization\n",
    "from utils import apply_codecut_style\n",
    "\n",
    "# Create a simple bar plot\n",
    "ax = analysis.pivot_table(\n",
    "    values=\"avg_amount\", index=\"product\", columns=\"region\"\n",
    ").plot(kind=\"bar\", color=[\"#72BEFA\", \"#E583B6\"])\n",
    "\n",
    "ax.set_title(\"Average Sales Amount by Product and Region\")\n",
    "ax.set_xlabel(\"Product\")\n",
    "ax.set_ylabel(\"Average Amount\")\n",
    "apply_codecut_style(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://codecut.ai/wp-content/uploads/2025/06/duckdb_plot-2.png)\n",
    "\n",
    "## Memory Efficiency {#memory-efficiency}\n",
    "\n",
    "A major drawback of pandas is its in-memory processing requirement. It must load complete datasets into RAM before any operations can begin, which can trigger out-of-memory errors when analyzing large datasets.\n",
    "\n",
    "To demonstrate this, we'll create a large dataset (1 million rows) and save it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample datasets\n",
    "n_rows = 1_000_000\n",
    "\n",
    "# Customers dataset\n",
    "customers = pd.DataFrame({\n",
    "    \"customer_id\": range(n_rows),\n",
    "    \"name\": [f\"Customer_{i}\" for i in range(n_rows)],\n",
    "    \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], n_rows),\n",
    "    \"segment\": np.random.choice([\"A\", \"B\", \"C\"], n_rows)\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "customers.to_csv(\"data/customers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter for customers in the 'North' region, pandas loads all 1 million customer records into RAM, even though we only need a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"data/customers.csv\")\n",
    "\n",
    "# Filter the data\n",
    "result = df[df[\"region\"] == \"North\"]\n",
    "print(result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "customer_id         name region segment\n",
    "9             9   Customer_9  North       B\n",
    "11           11  Customer_11  North       C\n",
    "17           17  Customer_17  North       B\n",
    "19           19  Customer_19  North       C\n",
    "23           23  Customer_23  North       B\n",
    "```\n",
    "\n",
    "Unlike pandas which loads the entire dataset into memory, DuckDB uses a columnar storage format and query optimization to only process the rows where `region = 'North'`.\n",
    "\n",
    "![](https://codecut.ai/wp-content/uploads/2025/06/pandas-vs-duckdb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Query a CSV file directly\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM 'data/customers.csv'\n",
    "    WHERE region = 'North'\n",
    "\"\"\").df()\n",
    "print(result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "customer_id         name region segment\n",
    "9             9   Customer_9  North       B\n",
    "11           11  Customer_11  North       C\n",
    "17           17  Customer_17  North       B\n",
    "19           19  Customer_19  North       C\n",
    "23           23  Customer_23  North       B\n",
    "```\n",
    "\n",
    "This selective loading approach significantly reduces memory usage, especially when working with large datasets.\n",
    "\n",
    "For building production-ready data science workflows that complement DuckDB's performance advantages, check out the complete book [Production-Ready Data Science: From Prototyping to Production with Python](https://codecut.ai/production-ready-data-science/).\n",
    "\n",
    "\n",
    "## Fast Performance {#fast-performance}\n",
    "\n",
    "While pandas processes data sequentially row-by-row, DuckDB uses a vectorized execution engine that processes data in parallel chunks. This architectural difference enables DuckDB to significantly outperform pandas, especially for computationally intensive operations like aggregations and joins.\n",
    "\n",
    "Let's compare the performance of pandas and DuckDB for aggregations on a million rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pandas aggregation\n",
    "start_time = time.time()\n",
    "pandas_agg = customers.groupby([\"region\", \"segment\"]).size().reset_index(name=\"count\")\n",
    "pandas_time = time.time() - start_time\n",
    "\n",
    "# DuckDB aggregation\n",
    "start_time = time.time()\n",
    "duckdb_agg = duckdb.sql(\"\"\"\n",
    "    SELECT region, segment, COUNT(*) as count FROM customers GROUP BY region, segment\n",
    "\"\"\").df()\n",
    "duckdb_time = time.time() - start_time\n",
    "\n",
    "# Print the results\n",
    "print(f\"Pandas aggregation time: {pandas_time:.2f} seconds\")\n",
    "print(f\"DuckDB aggregation time: {duckdb_time:.2f} seconds\")\n",
    "print(f\"Speedup: {pandas_time/duckdb_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Pandas aggregation time: 0.13 seconds\n",
    "DuckDB aggregation time: 0.02 seconds\n",
    "Speedup: 8.7x\n",
    "```\n",
    "\n",
    "We can see that DuckDB is much faster than pandas for aggregations. The performance difference becomes even more pronounced when working with larger datasets.\n",
    "\n",
    "## Streamlined File Reading {#streamlined-file-reading}\n",
    "\n",
    "### Automatic Parsing of CSV Files {#automatic-parsing-of-csv-files}\n",
    "\n",
    "When working with CSV files that have non-standard delimiters, you need to specify key parameters like delimiter and header with pandas to avoid parsing errors.\n",
    "\n",
    "To demonstrate this, let's create a CSV file with a custom delimiter and header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example CSV content with a custom delimiter\n",
    "csv_content = \"\"\"FlightDate|UniqueCarrier|OriginCityName|DestCityName\n",
    "1988-01-01|AA|New York, NY|Los Angeles, CA\n",
    "1988-01-02|AA|New York, NY|Los Angeles, CA\n",
    "1988-01-03|AA|New York, NY|Los Angeles, CA\n",
    "\"\"\"\n",
    "\n",
    "## Writing the CSV content to a file\n",
    "with open(\"data/flight_data.csv\", \"w\") as f:\n",
    "    f.write(csv_content)\n",
    "\n",
    "## Reading the CSV file with pandas without specifying the delimiter\n",
    "df = pd.read_csv(\"data/flight_data.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "                                      FlightDate|UniqueCarrier|OriginCityName|DestCityName\n",
    "1988-01-01|AA|New York NY|Los Angeles                                                 CA  \n",
    "1988-01-02|AA|New York NY|Los Angeles                                                 CA  \n",
    "1988-01-03|AA|New York NY|Los Angeles    \n",
    "```\n",
    "\n",
    "The output shows that pandas assumed the default delimiter (,) and incorrectly parsed the data into a single column.\n",
    "\n",
    "DuckDB's `read_csv` feature automatically detects the structure of the CSV file, including delimiters, headers, and column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "## Use DuckDB to automatically detect and read the CSV structure\n",
    "result = duckdb.query(\"SELECT * FROM read_csv('data/flight_data.csv')\").to_df()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "FlightDate UniqueCarrier OriginCityName     DestCityName\n",
    "0 1988-01-01            AA   New York, NY  Los Angeles, CA\n",
    "1 1988-01-02            AA   New York, NY  Los Angeles, CA\n",
    "2 1988-01-03            AA   New York, NY  Los Angeles, CA\n",
    "```\n",
    "\n",
    "The output shows that DuckDB automatically detected the correct delimiter (`|`) and correctly parsed the data into columns.\n",
    "\n",
    "### Automatic Flattening of Nested Parquet Files {#automatic-flattening-of-nested-parquet-files}\n",
    "\n",
    "When working with large, nested Parquet files, you typically need to pre-process the data to flatten nested structures or write complex extraction scripts, which adds time and complexity to your workflow.\n",
    "\n",
    "To demonstrate this, let's create a nested dataset and save it as a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a nested dataset and save it as a Parquet file\n",
    "data = {\n",
    "    \"id\": [1, 2],\n",
    "    \"details\": [{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}],\n",
    "}\n",
    "\n",
    "## Convert to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as a nested Parquet file\n",
    "df.to_parquet(\"data/customers.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To flatten the nested data with pandas, you need to create a new DataFrame with the flattened structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the DataFrame from Parquet file\n",
    "df = pd.read_parquet(\"data/customers.parquet\")\n",
    "\n",
    "# Create a new DataFrame with the flattened structure\n",
    "flat_df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": df[\"id\"],\n",
    "        \"name\": [detail[\"name\"] for detail in df[\"details\"]],\n",
    "        \"age\": [detail[\"age\"] for detail in df[\"details\"]],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(flat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id   name  age\n",
    "0   1  Alice   25\n",
    "1   2    Bob   30\n",
    "```\n",
    "\n",
    "DuckDB allows you to query nested Parquet files directly using SQL without needing to flatten or preprocess the data. This is much more efficient than pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "## Query the nested Parquet file directly\n",
    "query_result = duckdb.query(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        id,\n",
    "        details.name AS name,\n",
    "        details.age AS age\n",
    "    FROM read_parquet('data/customers.parquet')\n",
    "\"\"\"\n",
    ").to_df()\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id   name  age\n",
    "0   1  Alice   25\n",
    "1   2    Bob   30\n",
    "```\n",
    "\n",
    "\n",
    "In this example:\n",
    "\n",
    "- `read_parquet('customers.parquet')` reads the nested Parquet file.\n",
    "- SQL syntax allows you to access nested fields using dot notation (e.g., `details.name`).\n",
    "\n",
    "The output is a flattened representation of the nested data, directly queried from the Parquet file without additional preprocessing.\n",
    "\n",
    "### Automatic Flattening of Nested JSON Files {#automatic-flattening-of-nested-json-files}\n",
    "\n",
    "When working with JSON files that have nested structures, you need to normalize the data with pandas to access nested fields.\n",
    "\n",
    "To demonstrate this, let's create a nested JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample JSON data\n",
    "n_rows = 5\n",
    "\n",
    "# Generate nested JSON data\n",
    "data = []\n",
    "for i in range(n_rows):\n",
    "    record = {\n",
    "        \"user_id\": i,\n",
    "        \"profile\": {\"name\": f\"User_{i}\", \"active\": np.random.choice([True, False])},\n",
    "    }\n",
    "    data.append(record)\n",
    "\n",
    "# Save as JSON file\n",
    "with open(\"data/users.json\", \"w\") as f:\n",
    "    json.dump(data, f, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sample of the JSON file:\n",
    "\n",
    "```json\n",
    "[{'profile': {'active': 'False', 'name': 'User_0'}, 'user_id': 0},\n",
    " {'profile': {'active': 'True', 'name': 'User_1'}, 'user_id': 1},\n",
    " {'profile': {'active': 'False', 'name': 'User_2'}, 'user_id': 2},\n",
    " {'profile': {'active': 'True', 'name': 'User_3'}, 'user_id': 3},\n",
    " {'profile': {'active': 'False', 'name': 'User_4'}, 'user_id': 4}]\n",
    "```\n",
    "\n",
    "\n",
    "When working with nested JSON data in pandas, you'll need to use `pd.json_normalize` to flatten the nested structure. This step is required to access nested fields in a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON with pandas\n",
    "import pandas as pd\n",
    "\n",
    "df_normalized = pd.json_normalize(\n",
    "    data,\n",
    "    meta=[\"user_id\", [\"profile\", \"name\"], [\"profile\", \"active\"]],\n",
    ")\n",
    "\n",
    "print(\"Normalized data:\")\n",
    "print(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Normalized data:\n",
    "   user_id profile.name profile.active\n",
    "0        0       User_0          False\n",
    "1        1       User_1           True\n",
    "2        2       User_2          False\n",
    "3        3       User_3           True\n",
    "4        4       User_4          False\n",
    "```\n",
    "\n",
    "With DuckDB, you can query each nested field directly with the syntax `field_name.nested_field_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Query JSON directly\n",
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        user_id,\n",
    "        profile.name,\n",
    "        profile.active\n",
    "    FROM read_json('data/users.json')\n",
    "\"\"\"\n",
    ").df()\n",
    "\n",
    "print(\"JSON query results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "JSON query results:\n",
    "   user_id    name active\n",
    "0        0  User_0  False\n",
    "1        1  User_1   True\n",
    "2        2  User_2  False\n",
    "3        3  User_3   True\n",
    "4        4  User_4  False\n",
    "```\n",
    "\n",
    "## Reading Multiple Files {#reading-multiple-files}\n",
    "\n",
    "### Reading Multiple Files from a Directory {#reading-multiple-files-from-a-directory}\n",
    "\n",
    "It can be complicated to read multiple files from a folder with pandas.\n",
    "\n",
    "Let's say we have two CSV files in the `data/sales` directory. One containing January sales data and another with February sales data. Our goal is to combine both files into a single DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create example dataframe for first file\n",
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n",
    "        \"Product\": [\"Laptop\", \"Phone\", \"Tablet\"],\n",
    "        \"Sales\": [1200, 800, 600],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create example dataframe for second file\n",
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": [\"2023-02-01\", \"2023-02-02\", \"2023-02-03\"],\n",
    "        \"Product\": [\"Laptop\", \"Monitor\", \"Mouse\"],\n",
    "        \"Sales\": [1500, 400, 50],\n",
    "    }\n",
    ")\n",
    "\n",
    "Path(\"data/sales\").mkdir(parents=True, exist_ok=True)\n",
    "df1.to_csv(\"data/sales/jan.csv\", index=False)\n",
    "df2.to_csv(\"data/sales/feb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pandas, you need to read each file separately then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Read each file separately\n",
    "df1 = pd.read_csv(\"data/sales/jan.csv\")\n",
    "df2 = pd.read_csv(\"data/sales/feb.csv\")\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "df = pd.concat([df1, df2])\n",
    "print(df.sort_values(by=\"Date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Date  Product  Sales\n",
    "0  2023-01-01   Laptop   1200\n",
    "1  2023-01-02    Phone    800\n",
    "2  2023-01-03   Tablet    600\n",
    "0  2023-02-01   Laptop   1500\n",
    "1  2023-02-02  Monitor    400\n",
    "2  2023-02-03    Mouse     50\n",
    "```\n",
    "\n",
    "With DuckDB, you can read all the files in the `data/sales` folder at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "## Read and analyze all CSV files at once\n",
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM 'data/sales/*.csv'\n",
    "\"\"\"\n",
    ").df()\n",
    "print(result.sort_values(by=\"Date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Date  Product  Sales\n",
    "3 2023-01-01   Laptop   1200\n",
    "4 2023-01-02    Phone    800\n",
    "5 2023-01-03   Tablet    600\n",
    "0 2023-02-01   Laptop   1500\n",
    "1 2023-02-02  Monitor    400\n",
    "2 2023-02-03    Mouse     50\n",
    "```\n",
    "\n",
    "The result is a single DataFrame with all the data from both files.\n",
    "\n",
    "### Read From Multiple Sources {#read-from-multiple-sources}\n",
    "\n",
    "DuckDB allows you to read data from multiple sources in a single query, making it easier to combine data from different sources.\n",
    "\n",
    "To demonstrate, let's create some sample data in two different formats: a CSV file and a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample data in different formats\n",
    "n_rows = 5\n",
    "\n",
    "# 1. Create a CSV file with customer data\n",
    "customers = pd.DataFrame(\n",
    "    {\n",
    "        \"customer_id\": range(n_rows),\n",
    "        \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], n_rows),\n",
    "    }\n",
    ")\n",
    "customers.to_csv(\"data/customers.csv\", index=False)\n",
    "\n",
    "# 2. Create a Parquet file with order data\n",
    "orders = pd.DataFrame(\n",
    "    {\n",
    "        \"order_id\": range(n_rows),\n",
    "        \"customer_id\": np.random.randint(0, n_rows, n_rows),\n",
    "        \"amount\": np.random.normal(100, 30, n_rows),\n",
    "    }\n",
    ")\n",
    "orders.to_parquet(\"data/orders.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's query across all these sources in a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Query combining data from CSV and Parquet\n",
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        c.region,\n",
    "        COUNT(DISTINCT c.customer_id) as unique_customers,\n",
    "        AVG(o.amount) as avg_order_amount,\n",
    "        SUM(o.amount) as total_revenue\n",
    "    FROM 'data/customers.csv' c\n",
    "    JOIN 'data/orders.parquet' o\n",
    "        ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.region\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    ").df()\n",
    "\n",
    "print(\"Sales Analysis by Region:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a single DataFrame with the sales data from both files.\n",
    "\n",
    "## Parameterized Queries {#parameterized-queries}\n",
    "\n",
    "When working with databases, you often need to run similar queries with different parameters. For instance, you might want to filter a table using various criteria.\n",
    "\n",
    "Let's demonstrate this by creating an accounts table and inserting some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Create a new database file\n",
    "conn = duckdb.connect(\"data/bank.db\")\n",
    "\n",
    "# Create accounts table\n",
    "conn.sql(\n",
    "    \"\"\"\n",
    "    CREATE TABLE accounts (account_id INTEGER, name VARCHAR, balance DECIMAL(10,2))\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Insert sample accounts\n",
    "conn.sql(\n",
    "    \"\"\"\n",
    "    INSERT INTO accounts VALUES(1, 'Alice', 1000.00), (2, 'Bob', 500.00), (3, 'Charlie', 750.00)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you might want to search for accounts by different names or filter by different balance thresholds and reuse the same query multiple times.\n",
    "\n",
    "You can do this by using f-strings to pass parameters to your queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Open a connection\n",
    "conn = duckdb.connect(\"data/bank.db\")\n",
    "\n",
    "name_1 = \"A\"\n",
    "balance_1 = 100\n",
    "\n",
    "name_2 = \"C\"\n",
    "balance_2 = 200\n",
    "\n",
    "# Execute a query with parameters\n",
    "result_1 = conn.sql(\n",
    "    f\"SELECT * FROM accounts WHERE starts_with(name, '{name_1}') AND balance > {balance_1}\"\n",
    ").df()\n",
    "\n",
    "result_2 = conn.sql(\n",
    "    f\"SELECT * FROM accounts WHERE starts_with(name, '{name_2}') AND balance > {balance_2}\"\n",
    ").df()\n",
    "\n",
    "print(f\"result_1:\\n{result_1}\")\n",
    "print(f\"result_2:\\n{result_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this works, it's not a good idea as it can lead to SQL injection attacks.\n",
    "\n",
    "DuckDB provides safer ways to add parameters to your queries with parameterized queries.\n",
    "\n",
    "Parameterized queries allow you to pass values as arguments to your queries using the `execute` method and the `?` placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = conn.execute(\n",
    "    \"SELECT * FROM accounts WHERE starts_with(name, ?) AND balance > ?\",\n",
    "    (name_1, balance_1),\n",
    ").df()\n",
    "\n",
    "result_2 = conn.execute(\n",
    "    \"SELECT * FROM accounts WHERE starts_with(name, ?) AND balance > ?\",\n",
    "    (name_2, balance_2),\n",
    ").df()\n",
    "\n",
    "print(f\"result_1:\\n{result_1}\")\n",
    "print(f\"result_2:\\n{result_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACID Transactions {#acid-transactions}\n",
    "\n",
    "DuckDB supports ACID transactions on your data. Here are the properties of ACID transactions:\n",
    "\n",
    "- **Atomicity**: The transaction either completes entirely or has no effect at all. If any operation fails, all changes are rolled back.\n",
    "- **Consistency**: The database maintains valid data by enforcing all rules and constraints throughout the transaction.\n",
    "- **Isolation**: Transactions run independently without interfering with each other\n",
    "- **Durability**: Committed changes are permanent and survive system failures\n",
    "\n",
    "Let's demonstrate ACID properties with a bank transfer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Open a connection\n",
    "conn = duckdb.connect(\"data/bank.db\")\n",
    "\n",
    "\n",
    "def transfer_money(from_account, to_account, amount):\n",
    "    # Begin transaction\n",
    "    conn.sql(\"BEGIN TRANSACTION\")\n",
    "\n",
    "    # Check balance\n",
    "    balance = conn.execute(\n",
    "        \"SELECT balance FROM accounts WHERE account_id = ?\", (from_account,)\n",
    "    ).fetchone()[0]\n",
    "\n",
    "    if balance >= amount:\n",
    "        # Deduct money\n",
    "        conn.execute(\n",
    "            \"UPDATE accounts SET balance = balance - ? WHERE account_id = ?\",\n",
    "            (amount, from_account),\n",
    "        )\n",
    "\n",
    "        # Add money\n",
    "        conn.execute(\n",
    "            \"UPDATE accounts SET balance = balance + ? WHERE account_id = ?\",\n",
    "            (amount, to_account),\n",
    "        )\n",
    "\n",
    "        # Commit transaction\n",
    "        conn.sql(\"COMMIT\")\n",
    "    else:\n",
    "        # Rollback transaction\n",
    "        conn.sql(\"ROLLBACK\")\n",
    "        print(f\"Insufficient funds: {balance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above:\n",
    "\n",
    "- `BEGIN TRANSACTION` starts a transaction block where all SQL statements are grouped together as one unit. Changes remain hidden until committed.\n",
    "\n",
    "- `COMMIT` finalizes a transaction by permanently saving all changes made within the transaction block to the database. After a commit, the changes become visible to other transactions and cannot be rolled back.\n",
    "\n",
    "- `ROLLBACK` cancels all changes made within the current transaction block and restores the database to its state before the transaction began. This is useful for handling errors or invalid operations, ensuring data consistency.\n",
    "\n",
    "Let's try this out with a valid transfer and an invalid transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show initial balances\n",
    "print(\"Initial balances:\")\n",
    "print(conn.sql(\"SELECT * FROM accounts\").df())\n",
    "\n",
    "# Perform a valid transfer\n",
    "print(\"\\nPerforming valid transfer of $200 from Alice to Bob:\")\n",
    "transfer_money(1, 2, 200)\n",
    "\n",
    "# Show balances after valid transfer\n",
    "print(\"\\nBalances after valid transfer:\")\n",
    "print(conn.sql(\"SELECT * FROM accounts\").df())\n",
    "\n",
    "# Attempt an invalid transfer (insufficient funds)\n",
    "print(\"\\nAttempting invalid transfer of $1000 from Bob to Charlie:\")\n",
    "transfer_money(2, 3, 1000)\n",
    "\n",
    "# Show balances after failed transfer (should be unchanged)\n",
    "print(\"\\nBalances after failed transfer (should be unchanged):\")\n",
    "print(conn.sql(\"SELECT * FROM accounts\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bank transfer example demonstrates all ACID properties:\n",
    "\n",
    "- **Atomicity**: If the transfer fails (e.g., insufficient funds), both the deduction from Alice's account and addition to Bob's account are rolled back. The money is never lost in transit.\n",
    "- **Consistency**: The total balance of $1500 remains constant across all accounts. When Alice transfers $200 to Bob, her balance decreases by $200 and Bob's increases by $200.\n",
    "- **Isolation**: If multiple transfers happen simultaneously (e.g., Alice sending money to Bob while Charlie sends money to Alice), they won't interfere with each other. Each sees the correct account balances.\n",
    "- **Durability**: After the successful $200 transfer from Alice to Bob, their new balances ($300 and $700) are permanently saved, even if the system crashes.\n",
    "\n",
    "This makes DuckDB suitable for:\n",
    "\n",
    "- Financial applications\n",
    "- Data integrity critical operations\n",
    "- Concurrent data processing\n",
    "- Reliable data updates\n",
    "\n",
    "## Extensible {#extensible}\n",
    "\n",
    "DuckDB offers a rich set of extensions that can be used to add additional functionality to the database. \n",
    "\n",
    "To demonstrate the full-text search extension, let's create a table with articles and insert some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Create a connection\n",
    "conn = duckdb.connect(\"data/articles.db\")\n",
    "\n",
    "# Create a table with articles\n",
    "conn.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE articles (\n",
    "        article_id VARCHAR,\n",
    "        title VARCHAR,\n",
    "        content VARCHAR,\n",
    "        publish_date DATE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample articles\n",
    "conn.sql(\"\"\"\n",
    "    INSERT INTO articles VALUES\n",
    "        ('art1', 'Introduction to DuckDB',\n",
    "         'DuckDB is an embedded analytical database that supports SQL queries on local files.',\n",
    "         '2024-01-15'),\n",
    "        ('art2', 'Working with Large Datasets',\n",
    "         'Learn how to efficiently process large datasets using DuckDB and its powerful features.',\n",
    "         '2024-02-01'),\n",
    "        ('art3', 'SQL Performance Tips',\n",
    "         'Optimize your SQL queries for better performance in analytical workloads.',\n",
    "         '2024-02-15')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the full-text search capabilities. First, we need to create a full-text search index on the articles table using the `PRAGMA create_fts_index` function. This function takes:\n",
    "\n",
    "- `input_table`: The name of the table to create the full-text search index on.\n",
    "- `input_id`: The name of the column to use as the unique identifier for each row.\n",
    "- `*input_values`: The names of the columns to include in the full-text search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.sql(\"\"\"\n",
    "    PRAGMA create_fts_index('articles', 'article_id', 'title', 'content')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can search for articles about DuckDB by using the `fts_main_articles.match_bm25` function. This function takes:\n",
    "\n",
    "- `input_id`: The name of the column to use as the unique identifier for each row.\n",
    "- `query_string`: The search query to match against the full-text search index.\n",
    "- `fields`: The names of the columns to include in the full-text search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = conn.sql(\"\"\"\n",
    "    SELECT article_id, title, content, score\n",
    "    FROM (\n",
    "        SELECT *, fts_main_articles.match_bm25(article_id, 'DuckDB', fields := 'title,content') AS score\n",
    "        FROM articles\n",
    "    ) sq\n",
    "    WHERE score IS NOT NULL\n",
    "    ORDER BY score DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Articles about DuckDB:\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/khuyentran/codecut_content/codecut_articles/.venv/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
