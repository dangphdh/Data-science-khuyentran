{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started {#getting-started}\n",
    "\n",
    "First, install PySpark:\n",
    "\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "Create a SparkSession to start working with PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PySpark SQL Guide\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Create Sample Data\n",
    "\n",
    "First, let's create a sample CSV file that we'll use in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Create sample data.csv file\n",
    "data = [\n",
    "    [\"customer_id\", \"name\", \"region\", \"amount\"],\n",
    "    [1, \"Alice\", \"North\", 100],\n",
    "    [2, \"Bob\", \"South\", 150],\n",
    "    [3, \"Charlie\", \"North\", 200],\n",
    "    [4, \"Diana\", \"East\", 120],\n",
    "    [5, \"Eve\", \"West\", 180]\n",
    "]\n",
    "\n",
    "with open(\"data.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SparkSession is your entry point to all PySpark functionality.\n",
    "\n",
    "## Creating DataFrames {#creating-dataframes}\n",
    "\n",
    "PySpark supports creating DataFrames from multiple sources including Python objects, pandas DataFrames, files, and databases.\n",
    "\n",
    "Create from Python dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"customer_id\": [1, 2, 3, 4, 5],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"],\n",
    "    \"region\": [\"North\", \"South\", \"North\", \"East\", \"West\"],\n",
    "    \"amount\": [100, 150, 200, 120, 180]\n",
    "}\n",
    "\n",
    "# Create DataFrame from Python dictionary using zip and tuples\n",
    "df = spark.createDataFrame(\n",
    "    [(k, v1, v2, v3) for k, v1, v2, v3 in zip(\n",
    "        data[\"customer_id\"],\n",
    "        data[\"name\"],\n",
    "        data[\"region\"],\n",
    "        data[\"amount\"]\n",
    "    )],\n",
    "    [\"customer_id\", \"name\", \"region\", \"amount\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert from pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame(data)\n",
    "# Convert pandas DataFrame to PySpark DataFrame\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load from CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file with automatic schema inference\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read with schema specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"amount\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Load CSV file with explicit schema definition\n",
    "df = spark.read.csv(\"data.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Lazy Evaluation {#understanding-lazy-evaluation}\n",
    "\n",
    "PySpark's execution model differs fundamentally from pandas. Operations are divided into two types.\n",
    "\n",
    "Transformations are lazy operations that build execution plans without running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations return immediately\n",
    "filtered = df.filter(df.amount > 100)\n",
    "print(f\"Filtered: {filtered}\")\n",
    "\n",
    "selected = filtered.select(\"name\", \"amount\")\n",
    "print(f\"Selected: {selected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common transformations include `select()`, `filter()`, `withColumn()`, and `groupBy()`. They return instantly because they only build an execution plan and can be chained without performance cost.\n",
    "\n",
    "Actions trigger execution and return actual results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common actions include `show()`, `collect()`, `count()`, and `describe()`. They execute the entire chain of transformations and return actual results.\n",
    "\n",
    "This lazy evaluation enables Spark's [Catalyst optimizer](https://www.databricks.com/glossary/catalyst-optimizer) to analyze your complete workflow and apply optimizations like predicate pushdown and column pruning before execution.\n",
    "\n",
    "## Data Exploration {#data-exploration}\n",
    "\n",
    "Data exploration in PySpark works similarly to pandas, but with methods designed for distributed computing. Instead of pandas' `df.info()` and `df.head()`, PySpark uses `printSchema()` and `show()` to inspect schemas and preview records across the cluster.\n",
    "\n",
    "View the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics for numeric columns\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count total rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get distinct values in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique values in the region column\n",
    "df.select(\"region\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample random rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 60% of the rows\n",
    "df.sample(fraction=0.6, seed=42).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection & Filtering {#selection-filtering}\n",
    "\n",
    "When selecting and filtering data, PySpark uses explicit methods like `select()` and `filter()` that build distributed execution plans.\n",
    "\n",
    "Select specific columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns name and amount\n",
    "df.select(\"name\", \"amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter rows with conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where amount is greater than 150\n",
    "df.filter(df.amount > 150).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain multiple filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows where amount is greater than 100 and region is North\n",
    "(\n",
    "    df.filter(df.amount > 100)\n",
    "  .filter(df.region == \"North\")\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the customer_id column\n",
    "df.drop(\"customer_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Operations {#column-operations}\n",
    "\n",
    "Unlike pandas' mutable operations where `df['new_col']` modifies the DataFrame in place, PySpark's `withColumn()` and `withColumnRenamed()` return new DataFrames, maintaining the distributed computing model.\n",
    "\n",
    "The `withColumn()` method takes two arguments: the new column name and an expression defining its values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Add a new column with the amount with tax\n",
    "df.withColumn(\"amount_with_tax\", col(\"amount\") * 1.1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add constant value columns with `lit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Add a column with the same value for all rows\n",
    "df.withColumn(\"country\", lit(\"USA\")).select(\"name\", \"amount\", \"country\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `withColumnRenamed()` to rename a column by specifying the old name and new name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the amount column to revenue\n",
    "df.withColumnRenamed(\"amount\", \"revenue\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `cast()` method to convert a column to a different data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the amount column to a string\n",
    "df.withColumn(\"amount_str\", col(\"amount\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Functions {#aggregation-functions}\n",
    "\n",
    "Unlike pandas' in-memory aggregations, PySpark's `groupBy()` and aggregation functions distribute calculations across cluster nodes, using the same conceptual model as pandas but with lazy evaluation.\n",
    "\n",
    "Apply aggregation functions directly in `select()` to compute values across all rows without grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, count, max, min\n",
    "\n",
    "# Calculate total revenue, average revenue, and count across all rows\n",
    "df.select(\n",
    "    sum(\"amount\").alias(\"total_revenue\"),\n",
    "    avg(\"amount\").alias(\"avg_revenue\"),\n",
    "    count(\"*\").alias(\"order_count\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine `groupBy()` to create groups and `agg()` to compute multiple aggregations per group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total revenue and customer count by region\n",
    "(\n",
    "    df.groupBy(\"region\")\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"customer_count\")\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine `groupBy()` with `collect_list()` to create arrays of values for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# Collect customer names into an array for each region\n",
    "(\n",
    "    df.groupBy(\"region\")\n",
    "    .agg(collect_list(\"name\").alias(\"customers\"))\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Functions {#string-functions}\n",
    "\n",
    "Unlike pandas' vectorized string methods accessed via `.str`, PySpark provides importable functions like `concat()`, `split()`, and `regexp_replace()` that transform entire columns across distributed partitions.\n",
    "\n",
    "Use `concat()` to combine multiple columns and literal strings, wrapping constant values with `lit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "# Concatenate customer name and region with a separator\n",
    "df.withColumn(\"full_info\", concat(col(\"name\"), lit(\" - \"), col(\"region\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `split()` to divide a string column into an array based on a delimiter pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Create sample data with email addresses\n",
    "email_data = spark.createDataFrame(\n",
    "    [(\"alice@company.com\",), (\"bob@startup.io\",), (\"charlie@corp.net\",)],\n",
    "    [\"email\"]\n",
    ")\n",
    "\n",
    "# Split email into username and domain\n",
    "(\n",
    "    email_data.withColumn(\"email_parts\", split(col(\"email\"), \"@\"))\n",
    "    .select(\"email\", \"email_parts\")\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `regexp_replace()` to find and replace text patterns using regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Create sample data with phone numbers\n",
    "phone_data = spark.createDataFrame(\n",
    "    [(\"Alice\", \"123-456-7890\"), (\"Bob\", \"987-654-3210\"), (\"Charlie\", \"555-123-4567\")],\n",
    "    [\"name\", \"phone\"]\n",
    ")\n",
    "\n",
    "# Mask phone numbers, keeping only last 4 digits\n",
    "(\n",
    "    phone_data.withColumn(\"masked_phone\", regexp_replace(col(\"phone\"), r\"\\d{3}-\\d{3}-(\\d{4})\", \"XXX-XXX-$1\"))\n",
    "    .select(\"name\", \"phone\", \"masked_phone\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date/Time Functions {#datetime-functions}\n",
    "\n",
    "Working with dates and timestamps is essential for time-based analysis. PySpark offers comprehensive functions to extract date components, format timestamps, and perform temporal operations.\n",
    "\n",
    "Create sample data with dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "date_data = [\n",
    "    (1, datetime(2024, 1, 15), 100),\n",
    "    (2, datetime(2024, 2, 20), 150),\n",
    "    (3, datetime(2024, 3, 10), 200),\n",
    "    (4, datetime(2024, 4, 5), 120),\n",
    "    (5, datetime(2024, 5, 25), 180)\n",
    "]\n",
    "\n",
    "# Create sample DataFrame with datetime values\n",
    "df_dates = spark.createDataFrame(date_data, [\"id\", \"order_date\", \"amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use functions like `year()`, `month()`, and `dayofmonth()` to extract individual date components from timestamp columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "# Extract year, month, and day components from order_date\n",
    "(\n",
    "    df_dates.withColumn(\"year\", year(\"order_date\"))\n",
    "    .withColumn(\"month\", month(\"order_date\"))\n",
    "    .withColumn(\"day\", dayofmonth(\"order_date\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `date_format()` to convert dates to custom string formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# Format timestamps as YYYY-MM-DD strings\n",
    "(\n",
    "    df_dates.withColumn(\"formatted_date\", date_format(\"order_date\", \"yyyy-MM-dd\"))\n",
    "    .select(\"order_date\", \"formatted_date\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `to_timestamp()` to convert string columns to timestamp objects by specifying the date format pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "string_dates = spark.createDataFrame(\n",
    "    [(\"2024-01-15\",), (\"2024-02-20\",)],\n",
    "    [\"date_string\"]\n",
    ")\n",
    "\n",
    "# Convert date strings to timestamp objects\n",
    "string_dates.withColumn(\n",
    "    \"timestamp\",\n",
    "    to_timestamp(\"date_string\", \"yyyy-MM-dd\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Time Series {#working-with-time-series}\n",
    "\n",
    "Time series analysis often requires comparing values across different time periods. PySpark's window functions with lag and lead operations enable calculations of changes and trends over time.\n",
    "\n",
    "Create sample time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data = [\n",
    "    (1, datetime(2024, 1, 1), 100),\n",
    "    (1, datetime(2024, 1, 2), 120),\n",
    "    (1, datetime(2024, 1, 3), 110),\n",
    "    (2, datetime(2024, 1, 1), 200),\n",
    "    (2, datetime(2024, 1, 2), 220),\n",
    "    (2, datetime(2024, 1, 3), 210)\n",
    "]\n",
    "\n",
    "# Create time series data with multiple dates per customer\n",
    "df_ts = spark.createDataFrame(ts_data, [\"customer_id\", \"date\", \"amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the previous row's value within each customer group using `lag()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Create a window: group by customer_id, order by date\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"date\")\n",
    "\n",
    "# Get the previous amount for each customer\n",
    "df_ts.withColumn(\"prev_amount\", lag(\"amount\").over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row in each customer group has `null` for `prev_amount` because there's no previous value.\n",
    "\n",
    "Calculate day-over-day change by combining `lag()` to get the previous value and subtracting it from the current value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the day-over-day change in amount\n",
    "(\n",
    "    df_ts.withColumn(\"prev_amount\", lag(\"amount\", 1).over(window_spec))\n",
    "    .withColumn(\"daily_change\", col(\"amount\") - col(\"prev_amount\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Analytics {#window-analytics}\n",
    "\n",
    "Complex analytics operations like rankings, running totals, and moving averages require window functions that operate within data partitions. These functions enable sophisticated analytical queries without self-joins.\n",
    "\n",
    "Apply ranking functions within partitioned groups:\n",
    "\n",
    "- Combine `Window.partitionBy()` and `Window.orderBy()` to rank within groups\n",
    "- `rank()` handles ties by giving them the same rank with gaps (e.g., 1, 2, 2, 4)\n",
    "- `row_number()` always assigns unique sequential numbers (e.g., 1, 2, 3, 4)\n",
    "- `dense_rank()` gives ties the same rank without gaps (e.g., 1, 2, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank, row_number, dense_rank\n",
    "\n",
    "# Create sample data with categories to show ranking within groups\n",
    "ranking_data = spark.createDataFrame(\n",
    "    [(\"Math\", \"Alice\", 100), (\"Math\", \"Bob\", 150), (\"Math\", \"Charlie\", 150),\n",
    "     (\"Science\", \"Diana\", 200), (\"Science\", \"Eve\", 100)],\n",
    "    [\"subject\", \"name\", \"score\"]\n",
    ")\n",
    "\n",
    "# Define window partitioned by subject, ordered by score descending\n",
    "window_spec = Window.partitionBy(\"subject\").orderBy(col(\"score\").desc())\n",
    "\n",
    "# Calculate different ranking methods within each subject\n",
    "(\n",
    "    ranking_data.withColumn(\"rank\", rank().over(window_spec))\n",
    "    .withColumn(\"row_number\", row_number().over(window_spec))\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate running totals using `rowsBetween()` to define a window range:\n",
    "\n",
    "- `Window.unboundedPreceding` starts the window at the first row of the partition\n",
    "- `Window.currentRow` ends the window at the current row being processed\n",
    "- This creates an expanding window that includes all rows from the start up to the current position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# Create daily sales data with store identifier\n",
    "daily_sales = spark.createDataFrame(\n",
    "    [(\"A\", 1, 50), (\"A\", 2, 75), (\"A\", 3, 100),\n",
    "     (\"B\", 1, 25), (\"B\", 2, 150), (\"B\", 3, 80)],\n",
    "    [\"store\", \"day\", \"sales\"]\n",
    ")\n",
    "\n",
    "# Define window partitioned by store, from beginning to current row\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"store\")\n",
    "    .orderBy(\"day\")\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "# Calculate running total of sales per store\n",
    "daily_sales.withColumn(\"running_total\", _sum(\"sales\").over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `rowsBetween(-1, 1)` to create a 3-row sliding window that includes the previous row, current row, and next row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window for 3-row moving average (previous, current, next)\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"customer_id\")\n",
    "    .orderBy(\"date\")\n",
    "    .rowsBetween(-1, 1)\n",
    ")\n",
    "\n",
    "# Calculate moving average of amount over the window\n",
    "df_ts.withColumn(\"moving_avg\", avg(\"amount\").over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Operations {#join-operations}\n",
    "\n",
    "Combining data from multiple tables is a core operation in data analysis. PySpark supports various join types including inner, left, and broadcast joins, with automatic optimization for performance.\n",
    "\n",
    "Create sample tables for joining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample customers and orders tables for joining\n",
    "customers = spark.createDataFrame(\n",
    "    [(1, \"Alice\", \"US\"), (2, \"Bob\", \"UK\"), (3, \"Charlie\", \"US\")],\n",
    "    [\"customer_id\", \"name\", \"country\"]\n",
    ")\n",
    "\n",
    "orders = spark.createDataFrame(\n",
    "    [(101, 1, 100), (102, 2, 150), (103, 1, 200), (104, 3, 120)],\n",
    "    [\"order_id\", \"customer_id\", \"amount\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `join()` to perform an inner join, which returns only rows with matching keys in both DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join customers and orders on customer_id\n",
    "customers.join(orders, \"customer_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a left join by specifying `\"left\"` as the third argument, which retains all left table rows regardless of matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extended customers table including Diana\n",
    "customers_extended = spark.createDataFrame(\n",
    "    [(1, \"Alice\", \"US\"), (2, \"Bob\", \"UK\"), (3, \"Charlie\", \"US\"), (4, \"Diana\", \"CA\")],\n",
    "    [\"customer_id\", \"name\", \"country\"]\n",
    ")\n",
    "\n",
    "# Left join to keep all customers even without orders\n",
    "customers_extended.join(orders, \"customer_id\", \"left\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain multiple `join()` calls together to combine three or more DataFrames in sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create products and order_items tables\n",
    "products = spark.createDataFrame(\n",
    "    [(1, \"Widget\"), (2, \"Gadget\")],\n",
    "    [\"product_id\", \"product_name\"]\n",
    ")\n",
    "\n",
    "order_items = spark.createDataFrame(\n",
    "    [(101, 1), (102, 2), (103, 1), (104, 2)],\n",
    "    [\"order_id\", \"product_id\"]\n",
    ")\n",
    "\n",
    "# Chain multiple joins to combine orders, items, and products\n",
    "(\n",
    "    orders.join(order_items, \"order_id\")\n",
    "    .join(products, \"product_id\")\n",
    "    .select(\"order_id\", \"customer_id\", \"product_name\", \"amount\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Integration {#sql-integration}\n",
    "\n",
    "PySpark supports standard SQL syntax for querying data. You can write SQL queries using familiar SELECT, JOIN, and WHERE clauses alongside PySpark operations.\n",
    "\n",
    "Use `createOrReplaceTempView()` to register a DataFrame as a temporary SQL table, allowing it to be queried multiple times with SQL syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as a temporary SQL view named customers\n",
    "df.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute SQL queries on DataFrames registered with `createOrReplaceTempView()` using `spark.sql()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute SQL query to aggregate revenue by region\n",
    "spark.sql(\"\"\"\n",
    "    SELECT region, SUM(amount) as total_revenue\n",
    "    FROM customers\n",
    "    GROUP BY region\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain SQL queries with DataFrame operations by storing `spark.sql()` results and applying methods like `filter()` and `orderBy()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query customers with amount greater than 100\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT customer_id, name, amount\n",
    "    FROM customers\n",
    "    WHERE amount > 100\n",
    "\"\"\")\n",
    "\n",
    "# Filter SQL results using DataFrame API and sort by amount\n",
    "(\n",
    "    result.filter(col(\"region\") != \"South\")\n",
    "    .orderBy(\"amount\", ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SQL syntax within `spark.sql()` for complex joins and aggregations when SQL is more readable than DataFrame API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register orders and customers as temporary SQL views\n",
    "orders.createOrReplaceTempView(\"orders\")\n",
    "customers.createOrReplaceTempView(\"customers_table\")\n",
    "\n",
    "# Join and aggregate using SQL syntax\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        c.name,\n",
    "        COUNT(o.order_id) as order_count,\n",
    "        SUM(o.amount) as total_spent\n",
    "    FROM customers_table c\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.name\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions {#custom-functions}\n",
    "\n",
    "When built-in functions aren't sufficient, custom logic can be implemented using pandas UDFs. These user-defined functions provide vectorized performance through Apache Arrow and support both scalar operations and grouped transformations.\n",
    "\n",
    "Create a scalar pandas UDF with the `@pandas_udf` decorator to apply custom Python functions to columns with vectorized performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# Define pandas UDF to calculate discount based on price and quantity\n",
    "@pandas_udf(\"double\")\n",
    "def calculate_discount(amount: pd.Series, quantity: pd.Series) -> pd.Series:\n",
    "    return amount * quantity * 0.1\n",
    "\n",
    "# Create sample order data with price and quantity\n",
    "order_data = spark.createDataFrame(\n",
    "    [(1, 100.0, 2), (2, 150.0, 3), (3, 200.0, 1)],\n",
    "    [\"order_id\", \"price\", \"quantity\"]\n",
    ")\n",
    "\n",
    "# Apply discount calculation UDF to create discount column\n",
    "order_data.withColumn(\n",
    "    \"discount\",\n",
    "    calculate_discount(col(\"price\"), col(\"quantity\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply custom pandas functions to grouped data with `groupBy().applyInPandas()`:\n",
    "\n",
    "- Define a function that transforms each group as a pandas DataFrame\n",
    "- Specify output schema to tell PySpark the resulting column names and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to normalize amounts within each group\n",
    "def normalize_by_group(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    pdf[\"normalized\"] = (pdf[\"amount\"] - pdf[\"amount\"].mean()) / pdf[\"amount\"].std()\n",
    "    return pdf\n",
    "\n",
    "schema = \"customer_id long, date timestamp, amount long, normalized double\"\n",
    "\n",
    "# Apply normalization function to each customer_id group\n",
    "df_ts.groupBy(\"customer_id\").applyInPandas(normalize_by_group, schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Expressions {#sql-expressions}\n",
    "\n",
    "SQL expressions can be embedded directly within DataFrame operations for complex transformations. The `expr()`  and `selectExpr()` functions allow SQL syntax to be used alongside DataFrame methods, providing flexibility in query construction.\n",
    "\n",
    "Use the `expr()` function to embed SQL syntax within DataFrame operations, allowing SQL-style calculations in `withColumn()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Add tax and total columns using SQL expressions\n",
    "(\n",
    "    df.withColumn(\"tax\", expr(\"amount * 0.1\"))\n",
    "    .withColumn(\"total\", expr(\"amount + (amount * 0.1)\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike `select()` which uses column objects and method chaining, `selectExpr()` accepts SQL strings and is preferred for complex expressions that are simpler to write as SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with calculations and CASE statement using SQL syntax\n",
    "df.selectExpr(\n",
    "    \"customer_id\",\n",
    "    \"name\",\n",
    "    \"amount * 1.1 AS amount_with_tax\",\n",
    "    \"CASE WHEN amount > 150 THEN 'high' ELSE 'normal' END as category\"\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
