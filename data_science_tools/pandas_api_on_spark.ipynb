{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Scaling Pandas Workflows with PySpark's Pandas API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame({\"value\": [1, 2, 3, 4, 5]})\n",
    "print(pandas_df[\"value\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark_df = spark.createDataFrame([(1,), (2,), (3,), (4,), (5,)], [\"value\"])\n",
    "spark_df.select(avg(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Basic Operations Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "ps_s = ps.Series([1, 3, 5, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ps_df = pd.DataFrame(\n",
    "    {\"id\": np.arange(1, 1_000_001), \"value\": np.random.randn(1_000_000)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df_from_pandas = ps.from_pandas(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Data Exploration and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the summary of the DataFrame\n",
    "ps_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows and drop any NaN values\n",
    "filtered_df = ps_df.where(ps_df.value > 0).dropna()\n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df_2 = ps.DataFrame(\n",
    "    {\"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"], \"value\": [10, 20, 15, 30, 25]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Groupby Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df_2.groupby(\"category\").value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df[\"value\"].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df_2.plot.bar(x=\"category\", y=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df.to_csv(\"output_data.csv\", index=False)\n",
    "ps.read_csv(\"output_data.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## File I/O Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df.to_parquet(\"output_data.parquet\")\n",
    "ps.read_parquet(\"output_data.parquet\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a large Pandas API on Spark DataFrame\n",
    "large_pdf_df = ps.DataFrame(\n",
    "    {\n",
    "        \"feature1\": range(1_000_000),\n",
    "        \"feature2\": range(1_000_000, 2_000_000),\n",
    "        \"target\": range(500_000, 1_500_000),\n",
    "    }\n",
    ")\n",
    "print(f\"Length of the original DataFrame: {len(large_pdf_df):,}\")\n",
    "\n",
    "# Aggregate the data to a smaller size\n",
    "aggregated = large_pdf_df.groupby(large_pdf_df.feature1 // 10000).mean()\n",
    "print(f\"Length of the aggregated DataFrame: {len(aggregated):,}\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "small_pdf = aggregated.to_pandas()\n",
    "\n",
    "# Train a scikit-learn model\n",
    "model = LinearRegression()\n",
    "X = small_pdf[[\"feature1\", \"feature2\"]]\n",
    "y = small_pdf[\"target\"]\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Integration with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df[\"value\"] = pandas_df[\"value\"] + 1  # Operation executes immediately\n",
    "print(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Lazy vs Eager Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pandas API on Spark\n",
    "updated_psdf = ps_df.assign(a=ps_df[\"value\"] + 1)  # Lazy operation\n",
    "print(updated_psdf.head())  # Triggers actual computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "pyspark_df = spark.createDataFrame([(1, 4), (2, 5), (3, 6)], [\"col1\", \"col2\"])\n",
    "pyspark_df.select((col(\"col1\") + col(\"col2\")).alias(\"sum\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Advanced Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_spark_df = ps.DataFrame({\"col1\": [1, 2, 3], \"col2\": [4, 5, 6]})\n",
    "(pandas_spark_df[\"col1\"] + pandas_spark_df[\"col2\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas API on Spark DataFrame to PySpark DataFrame\n",
    "spark_native_df = pandas_spark_df.to_spark()\n",
    "\n",
    "# Now you can use full PySpark functionality\n",
    "spark_native_df.select((col(\"col1\") + col(\"col2\")).alias(\"sum\")).show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
