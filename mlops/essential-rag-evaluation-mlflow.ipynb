{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Production-Ready RAG Systems with MLflow Quality Metrics\n",
    "\n",
    "How do you know if your AI model actually works? AI model outputs can be inconsistent - sometimes providing inaccurate responses, irrelevant information, or answers that don't align with the input context. Manual evaluation of these issues is time-consuming and doesn't scale as your system grows.\n",
    "\n",
    "MLflow for GenAI solves this problem by automating evaluation across two critical areas: \n",
    "\n",
    "- Faithfulness: Ensuring responses match retrieved context\n",
    "- Answer Relevance: Verifying outputs address user questions\n",
    "\n",
    "This article teaches you to implement these evaluations and systematically improve your AI system's performance.\n",
    "\n",
    "## What is MLflow GenAI?\n",
    "\n",
    "[MLflow](https://mlflow.org/) is an open-source platform for managing machine learning lifecycles - tracking experiments, packaging models, and managing deployments. Traditional MLflow focuses on numerical metrics like accuracy and loss.\n",
    "\n",
    "MLflow for GenAI extends this foundation specifically for generative AI applications. It evaluates subjective qualities that numerical metrics can't capture:\n",
    "\n",
    "- **Response relevance**: Measures whether outputs address user questions\n",
    "- **Factual accuracy**: Checks if responses stay truthful to source material\n",
    "- **Context adherence**: Evaluates whether answers stick to retrieved information\n",
    "- **Automated scoring**: Uses AI judges instead of manual evaluation\n",
    "- **Scalable assessment**: Handles large datasets without human reviewers\n",
    "\n",
    "## Quick Setup\n",
    "\n",
    "### Installation\n",
    "\n",
    "Start by installing the necessary packages for this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from mlflow.metrics.genai import answer_relevance, faithfulness\n",
    "\n",
    "# Note: Ensure Ollama is installed and llama3.2 model is available\n",
    "# Run: ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG System with Ollama Llama3.2 {#rag-system-with-ollama-llama32}\n",
    "\n",
    "We'll create a real RAG (Retrieval-Augmented Generation) system using Ollama's Llama3.2 model that retrieves context and generates answers.\n",
    "\n",
    "This function creates a question-answering system that:\n",
    "\n",
    "- Takes a question and available documents as input\n",
    "- Uses the most relevant documents to provide context\n",
    "- Generates accurate answers using the Llama3.2 model\n",
    "- Returns both the answer and the sources used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_rag_system(question, context_docs):\n",
    "    \"\"\"Real RAG system using Ollama Llama3.2\"\"\"\n",
    "    # Retrieve top 2 most relevant documents\n",
    "    retrieved_context = \"\\n\".join(context_docs[:2])\n",
    "\n",
    "    # Create prompt template\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Answer the question based on the provided context.\n",
    "        Be concise and accurate.\n",
    "\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    # Initialize Llama3.2 model\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "    # Create chain and get response\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": retrieved_context, \"question\": question})\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"retrieved_docs\": context_docs[:2],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing vector databases with Pinecone, see our [Pinecone and Ollama semantic search guide](https://codecut.ai/pinecone-ollama-semantic-search-tutori/).\n",
    "\n",
    "### Evaluation Dataset {#evaluation-dataset}\n",
    "\n",
    "An evaluation dataset helps you measure system quality systematically. It reveals how well your RAG system handles different question types and identifies areas for improvement.\n",
    "\n",
    "To create an evaluation dataset, start with a knowledge base of documents that answer questions. Build the dataset with questions, expected answers, and context from this knowledge base.\n",
    "\n",
    "> For processing complex PDFs into RAG-ready data, explore our [Docling document processing guide](https://codecut.ai/docling-pdf-rag-document-processing/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base = [\n",
    "    \"MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides experiment tracking, model packaging, versioning, and deployment capabilities.\",\n",
    "    \"RAG systems combine retrieval and generation to provide accurate, contextual responses. They first retrieve relevant documents then generate answers.\",\n",
    "    \"Vector databases store document embeddings for efficient similarity search. They enable fast retrieval of relevant information.\"\n",
    "]\n",
    "\n",
    "eval_data = pd.DataFrame({\n",
    "    \"question\": [\n",
    "        \"What is MLflow?\",\n",
    "        \"How does RAG work?\",\n",
    "        \"What are vector databases used for?\"\n",
    "    ],\n",
    "    \"expected_answer\": [\n",
    "        \"MLflow is an open-source platform for managing machine learning workflows\",\n",
    "        \"RAG combines retrieval and generation for contextual responses\",\n",
    "        \"Vector databases store embeddings for similarity search\"\n",
    "    ],\n",
    "    \"context\": [\n",
    "        knowledge_base[0],\n",
    "        knowledge_base[1],\n",
    "        knowledge_base[2]\n",
    "    ]\n",
    "})\n",
    "\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Index | Question                            | Expected Answer                        | Context                                                  |\n",
    "|-------|-------------------------------------|----------------------------------------|----------------------------------------------------------|\n",
    "| 0     | What is MLflow?                     | Open-source ML workflow platform       | MLflow manages ML lifecycles with tracking, packaging... |\n",
    "| 1     | How does RAG work?                  | Combines retrieval and generation      | RAG systems retrieve documents then generate answers...  |\n",
    "| 2     | What are vector databases used for? | Store embeddings for similarity search | Vector databases enable fast retrieval of information... |\n",
    "\n",
    "Generate answers for each question using the RAG system. This creates the responses we'll evaluate for quality and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers for evaluation\n",
    "def generate_answers(row):\n",
    "    result = ollama_rag_system(row[\"question\"], [row[\"context\"]])\n",
    "    return result[\"answer\"]\n",
    "\n",
    "eval_data[\"generated_answer\"] = eval_data.apply(generate_answers, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first row to see the question, context, and generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first row to see question, context, and answer\n",
    "print(f\"Question: {eval_data.iloc[0]['question']}\")\n",
    "print(f\"Context: {eval_data.iloc[0]['context']}\")\n",
    "print(f\"Generated Answer: {eval_data.iloc[0]['generated_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output displays three key components:\n",
    "\n",
    "- The question shows what we asked.\n",
    "- The context shows which documents the system used to generate the answer.\n",
    "- The answer contains the RAG system's response.\n",
    "\n",
    "```text\n",
    "Question: What is MLflow?\n",
    "Context: MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides experiment tracking, model packaging, versioning, and deployment capabilities.\n",
    "Generated Answer: MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, providing features such as experiment tracking, model packaging, versioning, and deployment capabilities.\n",
    "```\n",
    "\n",
    "## Core RAG Metrics {#core-rag-metrics}\n",
    "\n",
    "### Faithfulness Evaluation {#faithfulness-evaluation}\n",
    "\n",
    "Faithfulness measures whether the generated answer stays true to the retrieved context, preventing hallucination:\n",
    "\n",
    "In the code below, we define the function `evaluate_faithfulness` that:\n",
    "\n",
    "- Creates an AI judge using GPT-4 to evaluate faithfulness.\n",
    "- Takes the generated answer, question, and context as input.\n",
    "- Returns a score from 1-5, where 5 indicates perfect faithfulness.\n",
    "\n",
    "We then apply this function to the evaluation dataset to get the faithfulness score for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate faithfulness for each answer\n",
    "def evaluate_faithfulness(row):\n",
    "    # Initialize faithfulness metric with OpenAI GPT-4 as judge\n",
    "    faithfulness_metric = faithfulness(model=\"openai:/gpt-4\")\n",
    "    score = faithfulness_metric(\n",
    "        predictions=[row[\"generated_answer\"]],\n",
    "        inputs=[row[\"question\"]],\n",
    "        context=[row[\"context\"]],\n",
    "    )\n",
    "    return score.scores[0]\n",
    "\n",
    "eval_data[\"faithfulness_score\"] = eval_data.apply(evaluate_faithfulness, axis=1)\n",
    "print(\"Faithfulness Evaluation Results:\")\n",
    "print(eval_data[[\"question\", \"faithfulness_score\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faithfulness Evaluation Results:\n",
    "\n",
    "| Question                            | Faithfulness Score |\n",
    "|-------------------------------------|--------------------|\n",
    "| What is MLflow?                     | 5                  |\n",
    "| How does RAG work?                  | 5                  |\n",
    "| What are vector databases used for? | 5                  |\n",
    "\n",
    "Perfect scores of 5 show the RAG system answers remain faithful to the source material. No hallucination or unsupported claims were detected.\n",
    "\n",
    "\n",
    "### Answer Relevance Evaluation {#answer-relevance-evaluation}\n",
    "\n",
    "Answer relevance measures whether the response actually addresses the question asked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate answer relevance\n",
    "def evaluate_relevance(row):\n",
    "    # Initialize answer relevance metric\n",
    "    relevance_metric = answer_relevance(model=\"openai:/gpt-4\")\n",
    "    score = relevance_metric(\n",
    "        predictions=[row[\"generated_answer\"]],\n",
    "        inputs=[row[\"question\"]]\n",
    "    )\n",
    "    return score.scores[0]\n",
    "\n",
    "eval_data[\"relevance_score\"] = eval_data.apply(evaluate_relevance, axis=1)\n",
    "print(\"Answer Relevance Results:\")\n",
    "print(eval_data[[\"question\", \"relevance_score\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer Relevance Results:\n",
    "\n",
    "| Question                            | Relevance Score |\n",
    "|-------------------------------------|-----------------|\n",
    "| What is MLflow?                     | 5               |\n",
    "| How does RAG work?                  | 5               |\n",
    "| What are vector databases used for? | 5               |\n",
    "\n",
    "Perfect scores of 5 show the RAG system's responses directly address the questions asked. No irrelevant or off-topic answers were generated.\n",
    "\n",
    "## Running and Interpreting Results {#running-and-interpreting-results}\n",
    "\n",
    "We'll now combine individual metrics into a comprehensive MLflow evaluation. This creates detailed reports, tracks experiments, and enables result comparison. Finally, we'll analyze the scores to identify areas for improvement.\n",
    "\n",
    "### Comprehensive Evaluation with MLflow {#comprehensive-evaluation-with-mlflow}\n",
    "\n",
    "Start by using MLflow's evaluation framework to run all metrics together.\n",
    "\n",
    "The following code:\n",
    "\n",
    "- Defines a model function that MLflow can evaluate systematically\n",
    "- Takes a DataFrame of questions and processes them through the RAG system\n",
    "- Converts results to a list format required by MLflow\n",
    "- Combines all metrics into a single evaluation run for comprehensive reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for MLflow evaluation\n",
    "def rag_model_function(input_df):\n",
    "    \"\"\"Model function for MLflow evaluation\"\"\"\n",
    "    def process_row(row):\n",
    "        result = ollama_rag_system(row[\"question\"], [row[\"context\"]])\n",
    "        return result[\"answer\"]\n",
    "\n",
    "    return input_df.apply(process_row, axis=1).tolist()\n",
    "\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "with mlflow.start_run() as run:\n",
    "    evaluation_results = mlflow.evaluate(\n",
    "        model=rag_model_function,\n",
    "        data=eval_data[\n",
    "            [\"question\", \"context\", \"expected_answer\"]\n",
    "        ],  # Include expected_answer column\n",
    "        targets=\"expected_answer\",\n",
    "        extra_metrics=[faithfulness_metric, relevance_metric],\n",
    "        evaluator_config={\n",
    "            \"col_mapping\": {\n",
    "                \"inputs\": \"question\",\n",
    "                \"context\": \"context\",\n",
    "                \"predictions\": \"predictions\",\n",
    "                \"targets\": \"expected_answer\",\n",
    "            }\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code, the evaluation results get stored in MLflow's tracking system. You can now compare different runs and analyze performance metrics through the dashboard.\n",
    "\n",
    "### Viewing Results in MLflow Dashboard {#viewing-results-in-mlflow-dashboard}\n",
    "\n",
    "Launch the MLflow UI to explore evaluation results interactively:\n",
    "\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "Navigate to `http://localhost:5000` to access the dashboard.\n",
    "\n",
    "The MLflow dashboard shows the Experiments table with two evaluation runs. Each run displays the run name (like \"bold-slug-816\"), creation time, dataset information, and duration. You can select runs to compare their performance metrics.\n",
    "\n",
    "![MLflow evaluation comparison interface showing side-by-side model performance metrics with scores for faithfulness and relevance](https://codecut.ai/wp-content/uploads/2025/07/compare.png)\n",
    "\n",
    "Click on any experiment to see the details of the evaluation. When you scroll down to the Metrics section, you will see detailed evaluation metrics including faithfulness and relevance scores for each question.\n",
    "\n",
    "![MLflow metrics dashboard displaying aggregated evaluation scores with average faithfulness and answer relevance ratings](https://codecut.ai/wp-content/uploads/2025/07/metrics.png)\n",
    "\n",
    "Clicking on \"Traces\" will show you the detailed request-response pairs for each evaluation question for debugging and analysis.\n",
    "\n",
    "![MLflow traces view showing detailed execution logs and evaluation chain for RAG model responses](https://codecut.ai/wp-content/uploads/2025/07/traces.png)\n",
    "\n",
    "Clicking on \"Artifacts\" reveals the evaluation results table containing the complete evaluation data, metric scores, and a downloadable format for external analysis.\n",
    "\n",
    "![MLflow artifacts panel displaying saved model outputs, evaluation datasets, and generated reports](https://codecut.ai/wp-content/uploads/2025/07/artifacts.png)\n",
    "\n",
    "### Interpreting the Results {#interpreting-the-results}\n",
    "\n",
    "Raw scores need interpretation to drive improvements. Use MLflow's evaluation data to identify specific areas for enhancement.\n",
    "\n",
    "The analysis:\n",
    "\n",
    "- Extracts performance metrics from comprehensive evaluation results\n",
    "- Calculates mean scores across all questions for both metrics\n",
    "- Identifies underperforming questions that require attention\n",
    "- Generates targeted feedback for systematic improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_evaluation_results(evaluation_results):\n",
    "    \"\"\"Analyze MLflow evaluation results\"\"\"\n",
    "\n",
    "    # Extract metrics and data\n",
    "    metrics = evaluation_results.metrics\n",
    "    eval_table = evaluation_results.tables[\"eval_results_table\"]\n",
    "\n",
    "    # Overall performance\n",
    "    avg_faithfulness = metrics.get(\"faithfulness/v1/mean\", 0)\n",
    "    avg_relevance = metrics.get(\"answer_relevance/v1/mean\", 0)\n",
    "\n",
    "    print(\"Average Scores:\")\n",
    "    print(f\"Faithfulness: {avg_faithfulness:.2f}\")\n",
    "    print(f\"Answer Relevance: {avg_relevance:.2f}\")\n",
    "\n",
    "    # Identify problematic questions\n",
    "    low_performing = eval_table[\n",
    "        (eval_table[\"faithfulness/v1/score\"] < 3) |\n",
    "        (eval_table[\"answer_relevance/v1/score\"] < 3)\n",
    "    ]\n",
    "\n",
    "    if not low_performing.empty:\n",
    "        print(f\"\\nQuestions needing improvement: {len(low_performing)}\")\n",
    "        for _, row in low_performing.iterrows():\n",
    "            print(f\"- {row['inputs']}\")\n",
    "    else:\n",
    "        print(\"\\nAll questions performing well!\")\n",
    "\n",
    "# Usage\n",
    "interpret_evaluation_results(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Average Scores:\n",
    "Faithfulness: 5.00\n",
    "Answer Relevance: 5.00\n",
    "\n",
    "All questions performing well!\n",
    "```\n",
    "\n",
    "Perfect scores indicate the RAG system generates accurate, contextual responses without hallucination. This baseline establishes a benchmark for future system modifications and more complex evaluation datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/khuyentran/codecut_content/codecut_articles/.venv/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
